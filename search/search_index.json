{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"REPACSS User Guide Overview","text":"<p>The REmotely-managed Power Aware Computing Systems and Services (REPACSS) is a high-performance computing (HPC) data center and AI infrastructure prototype that demonstrates the feasibility of using variable energy for advanced computing tasks, with the goal of reducing costs and improving efficiency. REPACSS is designed to support intensive computational and data-driven research, powered by variable energy sources. The system consists of a combination of compute, GPU, and storage nodes, interconnected with high-speed networking to ensure efficient data transfer and processing. The facility is structured to accommodate diverse workloads, including large-scale simulations, AI training, and data analytics. This documentation serves as the official reference for all users seeking to utilize REPACSS systems in accordance with established operational standards, security policies, and best practices. </p>"},{"location":"index.html#documentation-overview","title":"Documentation Overview","text":"<ul> <li> <p>Absolute Beginner\u2019s Guide   A high-level introduction to high-performance computing concepts and REPACSS usage for new users.</p> </li> <li> <p>Scheduling Policies and Compute Accounting   Definitions of resource allocation models, charge factors, runtime limits, and job prioritization policies.</p> </li> <li> <p>Sample SLURM Job Scripts   Verified examples to support the construction and submission of batch job scripts.</p> </li> <li> <p>Core Job Submission Procedures   Step-by-step overview of SLURM commands, job queues, and execution workflows.</p> </li> <li> <p>File Transfer Protocols and Utilities   Guidance on secure and efficient data transfers via Globus.</p> </li> <li> <p>UNIX File Access and Permissions   Overview of file ownership, group collaboration, and secure file access in a shared computing environment.</p> </li> <li> <p>Sharing File Access with Others   Guidance on how to share your files with others.</p> </li> <li> <p>Multi-Factor Authentication (MFA) Procedures   Official process for securing access to REPACSS resources using two-factor authentication.</p> </li> </ul>"},{"location":"index.html#system-infrastructure-and-technical-overview","title":"System Infrastructure and Technical Overview","text":"<ul> <li> <p>System Architecture   Technical description of the computational environment, including node types, CPUs, memory configurations, and interconnect topology.</p> </li> <li> <p>Known Issues and Hardware Anomalies   Documented performance irregularities and hardware-specific limitations.</p> </li> <li> <p>Software and Module Management   Procedures for loading, managing, and deploying software modules within the REPACSS environment.</p> </li> </ul>"},{"location":"index.html#resource-scheduling-and-job-execution","title":"Resource Scheduling and Job Execution","text":"<ul> <li>Batch and Interactive Job Submission </li> <li>Accessing Interactive Sessions </li> <li>Monitoring Job Performance and Troubleshooting </li> <li>Operational Best Practices for Job Submission</li> </ul>"},{"location":"index.html#optimization-and-performance-engineering","title":"Optimization and Performance Engineering","text":"<ul> <li>Compiler Flags </li> </ul>"},{"location":"index.html#reference-materials","title":"Reference Materials","text":"<ul> <li>Technical Glossary </li> <li>Error Message Catalog </li> <li>HPC Command Reference Sheet</li> </ul>"},{"location":"index.html#user-support","title":"User Support","text":"<ul> <li>Technical Support and Contact Information   Resources for user assistance, including help desk contact methods and ticket submission.</li> </ul>"},{"location":"index.html#contribution-and-governance","title":"Contribution and Governance","text":"<p>This documentation is actively maintained by the REPACSS Systems Administration and Support Team. All technical content is subject to periodic review to ensure compliance with institutional standards and research computing best practices.  </p> <p>Contributions, corrections, and feedback may be submitted via GitHub Pull Request, pending approval from the REPACSS documentation team.</p>"},{"location":"absolute-beginner-guide.html","title":"Introduction to REPACSS: A Beginner\u2019s Guide","text":"<p>About this page</p> <p>This document introduces the foundational steps for logging in, navigating storage environments, submitting jobs, and utilizing available software on the REPACSS cluster.</p> <p>Welcome to REPACSS \u2014 REmotely-managed Power Aware Computing Systems and Services at Texas Tech University. This guide is designed for new users, graduate researchers, including educators and others who are beginning their journey in high-performance computing (HPC). </p>","tags":["onboarding","login","ssh","storage","slurm","modules","cluster usage"]},{"location":"absolute-beginner-guide.html#accessing-the-system","title":"Accessing the System","text":"<p>Before accessing REPACSS resources, users must be connected to TTUnet or TTUnet VPN.   </p> <p>TTUnet VPN Usage Cases</p> <p>On Campus:  - You must be connected to TTUnet, either via wired Ethernet or the TTUnet Wi-Fi network.   - Other networks on campus (such as TTUguest or EduRoam) are not supported for direct access. Off Campus:  - If you are connecting from any other network, including TTUguest, EduRoam, or external internet connections, you must use the TTU GlobalProtect Virtual Private Network (VPN).  - Instructions are available on the VPN Setup Guide.  Authentication: All system access requires secure login via SSH or authorized web-based interfaces. Note: Users located within the Computer Science Department building may experience restricted access when using TTUnet Wi-Fi. If you encounter connectivity issues, connect via wired Ethernet or enable the VPN to ensure uninterrupted access.</p>","tags":["onboarding","login","ssh","storage","slurm","modules","cluster usage"]},{"location":"absolute-beginner-guide.html#ssh-login","title":"SSH Login","text":"<p>To initiate a session:</p> <pre><code>ssh &lt;your_username&gt;@repacss.ttu.edu\n</code></pre> <p>During first-time access, the system may prompt you to verify the server\u2019s RSA key fingerprint. Confirm by typing <code>yes</code>. You will then be required to enter your password.</p> <p>Note</p> <p>Login nodes are reserved for light activities such as file management and job preparation. Computational jobs must be executed on compute nodes.</p>","tags":["onboarding","login","ssh","storage","slurm","modules","cluster usage"]},{"location":"absolute-beginner-guide.html#system-overview","title":"System Overview","text":"","tags":["onboarding","login","ssh","storage","slurm","modules","cluster usage"]},{"location":"absolute-beginner-guide.html#login-node","title":"Login Node","text":"<p>Login node is intended for lightweight tasks only. Users should use it to:</p> <ul> <li>Edit and manage files  </li> <li>Install user-level software or modules  </li> <li>Compile code (if lightweight)  </li> <li>Submit SLURM job scripts  </li> </ul> <p>Warning</p> <p>Do not run compute-intensive applications or parallel jobs on the login node.</p>","tags":["onboarding","login","ssh","storage","slurm","modules","cluster usage"]},{"location":"absolute-beginner-guide.html#compute-nodes","title":"Compute Nodes","text":"<p>All computational jobs should be submitted to the compute nodes via SLURM. The system includes:</p> <ul> <li>110 CPU worker nodes for general-purpose parallel and serial computing  </li> <li>8 GPU worker nodes for accelerated workloads (e.g., deep learning, GPU-based simulations)  </li> <li>1 GPU build node for compiling and testing GPU applications in a controlled environment</li> </ul> <p>Any workload requiring high-performance, extended runtime, or parallel execution should be run on these compute nodes.</p>","tags":["onboarding","login","ssh","storage","slurm","modules","cluster usage"]},{"location":"absolute-beginner-guide.html#system-specifications-summary-table","title":"System Specifications Summary Table","text":"Node Type Total Nodes CPU Model CPUs/Node Cores/Node Memory/Node Storage/Node GPUs/Node GPU Model CPU Nodes 110 AMD EPYC 9754 2 256 1.5 TB DDR5 1.92 TB NVMe - - GPU Nodes 8 Intel Xeon Gold 6448Y 2 64 512 GB 1.92 TB SSD 4 NVIDIA H100 NVL (94 GB HBM) Login Nodes 3 AMD EPYC 9254 2 48 256 GB 1.92 TB NVMe - - Storage Nodes 9 Intel Xeon Gold (varied) 2 8\u201332 512 GB\u20131 TB 25.6\u2013583.68 TB - -","tags":["onboarding","login","ssh","storage","slurm","modules","cluster usage"]},{"location":"absolute-beginner-guide.html#storage-system","title":"Storage System","text":"<p>REPACSS offers multiple storage environments optimized for different use cases:</p> Storage Type Location Environment Variable Home <code>/mnt/GROUPID/home/USERID</code> $HOME Scratch <code>/mnt/GROUPID/scratch/USERID</code> $SCRATCH Work <code>/mnt/GROUPID/work/USERID</code> $WORK","tags":["onboarding","login","ssh","storage","slurm","modules","cluster usage"]},{"location":"absolute-beginner-guide.html#checking-quotas","title":"Checking Quotas","text":"<p>REPACSS storage space usage is currently organized by the REPACSS group.  Use the following command to display your current file usage:</p> <pre><code>$ df -h /mnt/$(id -gn)\n\nFilesystem              Size  Used Avail Use% Mounted on\n10.102.95.220:/REPACSS  9.1T  162G  9.0T   2% /mnt/REPACSS\n</code></pre>","tags":["onboarding","login","ssh","storage","slurm","modules","cluster usage"]},{"location":"absolute-beginner-guide.html#software-access-and-modules","title":"Software Access and Modules","text":"<p>REPACSS provides software access via the environment module system. This system allows users to load and unload software packages as needed.</p>","tags":["onboarding","login","ssh","storage","slurm","modules","cluster usage"]},{"location":"absolute-beginner-guide.html#common-module-commands","title":"Common Module Commands","text":"<pre><code>module avail            # List available software modules\nmodule load gcc         # Load a specific module\nmodule list             # View loaded modules\nmodule unload gcc       # Unload a module\n</code></pre> <p>Users should include required module commands at the beginning of their job scripts.</p> Load GCC module <p>For example, to load gcc in a job script:</p> <pre><code>module load gcc\n</code></pre> <p>For additonal details, refer to the Module System documentation.</p>","tags":["onboarding","login","ssh","storage","slurm","modules","cluster usage"]},{"location":"absolute-beginner-guide.html#job-submission-with-c-program","title":"Job Submission with C Program","text":"<p>This example demonstrates the procedure for compiling and executing a basic C program using a SLURM batch script.</p>","tags":["onboarding","login","ssh","storage","slurm","modules","cluster usage"]},{"location":"absolute-beginner-guide.html#create-your-source-code","title":"Create your source code","text":"<pre><code>#include &lt;stdio.h&gt;\n\nint main(){\n    printf(\"Hello from my SLURM job.\\n\");\n    return 0;\n}\n</code></pre>","tags":["onboarding","login","ssh","storage","slurm","modules","cluster usage"]},{"location":"absolute-beginner-guide.html#create-your-batch-scriptrun_hellosh","title":"Create your Batch Script(<code>run_hello.sh</code>)","text":"<p><pre><code>#!/bin/bash\n#SBATCH --job-name=hello_job\n#SBATCH --output=hello_output.out\n#SBATCH --error=hello_error.err\n#SBATCH --time=00:05:00\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=1\n\nmodule load gcc\n\ngcc hello.c -o hello\n\n./hello\n</code></pre> To determine your resource needs, refer to the Determine Resource Needs documentation.</p>","tags":["onboarding","login","ssh","storage","slurm","modules","cluster usage"]},{"location":"absolute-beginner-guide.html#submit-your-job","title":"Submit Your Job","text":"<p>To submit the batch script to the SLURM workload manager, execute the following command: <pre><code>sbatch run_hello.sh\n</code></pre></p> <p>How to monitor the job status?</p> <p>To monitor the status of the submitted job, use: <pre><code>squeue --me\n</code></pre></p> <p>For additonal details, refer to the Job Examples documentation.</p>","tags":["onboarding","login","ssh","storage","slurm","modules","cluster usage"]},{"location":"absolute-beginner-guide.html#support-and-resources","title":"Support and Resources","text":"<p>If issues arise or assistance is required, users are encouraged to:</p> <ul> <li>Refer to this user guide</li> <li>Consult with their research advisor or lab administrator</li> <li>Visit the Support Page</li> </ul> <p>Tip</p> <p>When seeking help, include specific error messages and a description of the attempted job to expedite troubleshooting.</p>","tags":["onboarding","login","ssh","storage","slurm","modules","cluster usage"]},{"location":"faq.html","title":"Frequently Asked Questions","text":""},{"location":"faq.html#account-and-access","title":"Account and Access","text":""},{"location":"faq.html#how-do-i-obtain-access-to-the-repacss-system","title":"How do I obtain access to the REPACSS system?","text":"<p>To gain access to the REPACSS high-performance computing environment, please follow the procedure outlined below:</p> <ol> <li>Contact the REPACSS support team by email to initiate an access request.</li> <li>Complete any mandatory training to demonstrate understanding of system usage policies and operational guidelines.</li> <li>Review and acknowledge the REPACSS user policy agreement.</li> <li>Await account approval and confirmation from system administrators.</li> </ol>"},{"location":"faq.html#how-do-i-reset-my-password","title":"How do I reset my password?","text":"<p>Password management is handled through the institutional eRaider account system. Users may visit https://eraider.ttu.edu to reset credentials. ACCESS users should consult their ACCESS identity management portal for related instructions.</p>"},{"location":"faq.html#job-submission-and-system-usage","title":"Job Submission and System Usage","text":""},{"location":"faq.html#how-do-i-submit-a-job","title":"How do I submit a job?","text":"<p>To submit batch jobs to the Slurm workload manager, use the following command structures:</p> <pre><code>sbatch job.sh\nsbatch -p zen4 job.sh\nsbatch -p h100 job.sh\n</code></pre>"},{"location":"faq.html#how-do-i-monitor-the-status-of-submitted-jobs","title":"How do I monitor the status of submitted jobs?","text":"<p>To monitor the status and queue placement of active or pending jobs, use the following commands:</p> <pre><code>squeue -u $USER\nsqueue -p zen4\nsqueue -p h100\n</code></pre>"},{"location":"faq.html#how-do-i-cancel-a-submitted-job","title":"How do I cancel a submitted job?","text":"<p>To terminate an active or pending job, the following commands may be used:</p> <pre><code>scancel &lt;job_id&gt;\nscancel -u $USER\nscancel -p zen4\n</code></pre>"},{"location":"faq.html#storage-and-data-management","title":"Storage and Data Management","text":""},{"location":"faq.html#how-do-i-transfer-files-to-and-from-the-repacss-system","title":"How do I transfer files to and from the REPACSS system?","text":"<p>Important Notice: It is strongly recommended to use Globus for large-scale or frequent data transfers. Secure Copy Protocol (SCP) may be used for infrequent or small transfers.</p> <pre><code># Upload to REPACSS\nscp file username@login.repacss.org:/mnt/GROUPID/work/USERID/\n\n# Download from REPACSS\nscp username@login.repacss.org:/mnt/GROUPID/work/USERID/file ./destination\n</code></pre>"},{"location":"faq.html#software-environment","title":"Software Environment","text":""},{"location":"faq.html#how-do-i-view-and-load-available-software-modules","title":"How do I view and load available software modules?","text":"<p>The Lmod module system is used to manage available software environments. Use the following commands to access and configure software:</p> <pre><code>module avail\nmodule load &lt;module-name&gt;\nmodule list\n</code></pre>"},{"location":"faq.html#how-do-i-request-the-installation-of-new-software","title":"How do I request the installation of new software?","text":"<p>To request the installation of additional software packages:</p> <ol> <li>Contact the REPACSS support team with a formal request.</li> <li>Include installation instructions, dependencies, and intended use case.</li> <li>Indicate whether the request is for shared usage or for individual access.</li> <li>Await review and approval by system administrators.</li> </ol>"},{"location":"getting-started-at-REPACSS.html","title":"Getting Started at REPACSS","text":"<p>About this page</p> <p>This page will guide you through the basics of accessing, connecting to, and beginning work on the REPACSS High-Performance Computing (HPC) cluster.</p> <p>Welcome to REPACSS \u2014 Texas Tech University's research and educational platform for advanced computing support services. This guide is tailored to new users who are preparing to run jobs on the cluster for the first time.</p>"},{"location":"getting-started-at-REPACSS.html#account-access","title":"Account Access","text":"<p>To utilize REPACSS resources, users must have a valid TTU account and a TTUnet VPN access.</p>"},{"location":"getting-started-at-REPACSS.html#ttu-users","title":"TTU Users","text":"<ul> <li>Access must be requested through the system administrator.</li> <li>A valid institutional email address (ending in <code>ttu.edu</code>) is required.</li> <li>Provisioning typically takes 2\u20133 business days after the request is approved.</li> </ul>"},{"location":"getting-started-at-REPACSS.html#multi-factor-authentication-mfa-and-vpn","title":"Multi-Factor Authentication (MFA) and VPN","text":"<p>All users accessing the system remotely must use TTU\u2019s GlobalProtect VPN and have Microsoft Multi-Factor Authentication (MFA) configured.</p> <ul> <li>VPN Setup Instructions</li> <li>MFA Configuration Guide</li> </ul> <p>MFA and VPN required</p> <p>Login attempts from outside the TTU network require both MFA and VPN authentication.</p>"},{"location":"getting-started-at-REPACSS.html#connecting-to-the-cluster","title":"Connecting to the Cluster","text":"<p>Once account provisioning and secure connection setup are complete, users may connect via Secure Shell (SSH):</p> <pre><code>ssh &lt;your-ttu-username&gt;@repacss.ttu.edu\n</code></pre> <p>Access via Visual Studio Code is also supported:</p> <ul> <li>Connecting via VSCode</li> </ul> <p>First-time login tip</p> <p>On initial login, run <code>quota -s</code> to review disk usage, and <code>module avail</code> to browse available software modules.</p> <p>Storage areas</p> <ul> <li>Use <code>/home</code> for scripts and long-term storage.</li> <li>Use <code>/work</code> for project-related data under active development.</li> <li>Use <code>/scratch</code> for temporary data during compute jobs.</li> </ul>"},{"location":"getting-started-at-REPACSS.html#environment-setup","title":"Environment Setup","text":"<p>REPACSS uses an environment module system to manage software packages.</p> <pre><code>module avail         # List available modules\nmodule load gcc/12   # Load a specific version\n</code></pre> <p>Resources:</p> <ul> <li>Using Modules</li> <li>Available Software</li> <li>Installing Your Own Packages</li> </ul>"},{"location":"getting-started-at-REPACSS.html#submitting-your-first-job","title":"Submitting Your First Job","text":"<p>REPACSS employs the SLURM workload manager to schedule jobs.   </p> <p>Tip</p> <p>Interactive sessions should be used exclusively for development and debugging purposes. Once your job is ready for full execution, please submit it using a SLURM batch job.</p>"},{"location":"getting-started-at-REPACSS.html#sample-slurm-script","title":"Sample SLURM Script","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=test\n#SBATCH --output=output.txt\n#SBATCH --ntasks=1\n#SBATCH --time=00:05:00\n\nsrun ./my_program\n</code></pre> <p>Submit the job with:</p> <pre><code>sbatch job.slurm\n</code></pre> <p>Monitor your job with:</p> <pre><code>squeue -u &lt;your-username&gt;\n</code></pre> <p>Resources:</p> <ul> <li>SLURM Job Basics</li> <li>Example Job Scripts</li> </ul>"},{"location":"getting-started-at-REPACSS.html#getting-help","title":"Getting Help","text":"<p>If assistance is required:</p> <ul> <li>Visit the Support Page</li> </ul>"},{"location":"status.html","title":"System Status","text":"<p>Warning</p> <p>This page is under development. Real-time system status and incident reporting features will be added soon.</p> <p>This page will provide live updates on system health, maintenance windows, and known issues affecting the REPACSS infrastructure.</p> <p>Stay tuned!</p>"},{"location":"support.html","title":"Support","text":""},{"location":"support.html#current-support-channels","title":"Current Support Channels","text":""},{"location":"support.html#contact-information","title":"Contact Information","text":"<ul> <li>Email: repacss.support@ttu.edu  </li> <li>Office Hours: TBD</li> <li>Emergency Support: TBD</li> </ul>"},{"location":"support.html#getting-help","title":"Getting Help","text":""},{"location":"support.html#use-the-documentation-first","title":"Use the Documentation First","text":"<p>Start by reviewing available documentation:</p> <ul> <li>Getting Started Guide</li> <li>Frequently Asked Questions (FAQ)</li> </ul> <p>Many questions are already answered in the guides, saving you time and allowing you to learn more about the system.</p>"},{"location":"support.html#check-the-faq","title":"Check the FAQ","text":"<p>If you can\u2019t find what you need in the documentation, check the FAQ. It contains quick solutions to commonly reported issues and system behaviors that may seem unclear.</p>"},{"location":"support.html#reaching-out-to-support","title":"Reaching Out to Support","text":"<p>If you still need assistance:</p> <ol> <li>Email <code>repacss.support@ttu.edu</code></li> <li>Provide the following:</li> <li>Your username</li> <li>Steps to reproduce the issue</li> <li>Any error messages or output logs</li> <li>What troubleshooting you've already attempted</li> </ol> <p>The more detail you provide, the faster we can help resolve your issue.</p>"},{"location":"support.html#related-resources","title":"Related Resources","text":"<ul> <li>FAQ</li> <li>Contact</li> </ul>"},{"location":"account/index.html","title":"Account Management","text":"<p>Purpose of This Section</p> <p>This section outlines procedures for managing user accounts on the REPACSS High-Performance Computing (HPC) system, including credential updates and authentication protocols.</p> <p>In order to access REPACSS resources, users must maintain an active and secure account. Proper account management ensures continued access to computing services, storage allocations, and user support systems.</p>"},{"location":"account/index.html#password-and-credential-management","title":"Password and Credential Management","text":"<p>Secure authentication is required for all user interactions with the REPACSS system. The following credential management practices are supported:</p> <ul> <li>Secure password updates through the official TTU identity management system</li> <li>Multi-Factor Authentication (MFA) support</li> <li>SSH key registration for secure and password-less logins</li> </ul> <p>If your password is forgotten or compromised, you must contact Texas Tech University\u2019s IT Help Central to initiate a reset and regain account access.</p>"},{"location":"account/index.html#support-and-issue-resolution","title":"Support and Issue Resolution","text":"<p>For assistance with account access, credential issues, or related technical matters, contact the REPACSS Support Team via email at repacss.support@ttu.edu. Include your eRaider ID and a detailed description of the issue to expedite resolution.</p>"},{"location":"account/access.html","title":"ACCESS Accounts","text":"<p>Feature Under Development \ud83d\udea7</p> <p>Integration with the ACCESS (Advanced Cyberinfrastructure Coordination Ecosystem: Services &amp; Support) is not enabled yet in REPACSS. We're actively working on bringing this feature online to support broader national CI access. Stay tuned!</p>"},{"location":"account/access.html#what-is-access","title":"What is ACCESS?","text":"<p>ACCESS is a U.S. National Science Foundation (NSF) program that provides researchers with access to advanced computational resources, support services, and training. REPACSS aims to integrate with ACCESS in the future to:</p> <ul> <li>Expand national access to REPACSS resources  </li> <li>Support collaboration across U.S. institutions  </li> </ul> <p>If you're already an ACCESS user and interested in future integration testing or early access, please contact us at repacss.support@ttu.edu</p>"},{"location":"account/passwords.html","title":"Account Password and Credential Management","text":"<p>Purpose of this Section</p> <p>This section outlines credential responsibilities, password recovery procedures, and authentication requirements for REPACSS system access.</p>"},{"location":"account/passwords.html#password-responsibility-and-usage-policy","title":"Password Responsibility and Usage Policy","text":"<p>Each user is issued a unique username and password to access the REPACSS computing environment. These credentials are strictly personal and must not be shared under any circumstances. Sharing account credentials is a violation of institutional security policy and may result in immediate suspension of system access.</p> <p>Users are expected to change their password immediately if a compromise is suspected. In such cases, notify the REPACSS support team at repacss.support@ttu.edu without delay.</p>"},{"location":"account/passwords.html#forgotten-password-procedures","title":"Forgotten Password Procedures","text":""},{"location":"account/passwords.html#ttu-eraider-linked-accounts","title":"TTU eRaider-Linked Accounts","text":"<p>For accounts authenticated via Texas Tech University\u2019s eRaider system:</p> <ol> <li>Navigate to the TTU eRaider Password Reset page.</li> <li>Select the option labeled \u201cForgot Password?\u201d.</li> <li>Enter the required identity verification details.</li> <li>Follow the reset instructions delivered to your registered email address.</li> </ol> <p>If additional assistance is required, contact TTU IT Help Central:</p> <ul> <li>Phone: 806-742-HELP (4357) </li> </ul>"},{"location":"account/passwords.html#changing-your-password","title":"Changing Your Password","text":""},{"location":"account/passwords.html#for-eraider-linked-accounts","title":"For eRaider-Linked Accounts","text":"<ol> <li>Visit https://eraider.ttu.edu</li> <li>Log in with your existing credentials</li> <li>Select the Change Password option</li> <li>Enter a new password that complies with TTU\u2019s minimum security standards</li> </ol>"},{"location":"account/passwords.html#password-policy-and-requirements","title":"Password Policy and Requirements","text":"<p>Password Policy</p> <p>Since all user accounts are linked to eRaider credentials, the policies and guidelines governing eRaider accounts shall apply. Users are expected to comply with Texas Tech University's eRaider account usage policy at all times when accessing the system.</p>"},{"location":"account/passwords.html#login-failures-and-lockout-policy","title":"Login Failures and Lockout Policy","text":"<p>User accounts are temporarily locked following 15 consecutive failed login attempts. This lock will automatically expire after 15 minutes. If you are still unable to access your account, please initiate a password reset or contact support.</p>"},{"location":"account/passwords.html#multi-factor-authentication-mfa","title":"Multi-Factor Authentication (MFA)","text":"<p>At present, Multi-Factor Authentication (MFA) is not required for direct REPACSS access. However, users accessing through TTU\u2019s GlobalProtect VPN or other university services may be subject to TTU\u2019s institutional MFA requirements.</p>"},{"location":"account/passwords.html#assistance-and-support","title":"Assistance and Support","text":"<p>For account assistance or additional help:</p> <ul> <li>Email: repacss.support@ttu.edu </li> <li>In Person: Engineering Center, Texas Tech University Campus</li> </ul>"},{"location":"account/policy.html","title":"Account Usage Policies","text":"<p>Purpose of This Document</p> <p>This document outlines mandatory account usage policies for all users of the REPACSS system at Texas Tech University. Adherence to these policies ensures the security, integrity, and proper utilization of REPACSS high-performance computing (HPC) resources.</p>"},{"location":"account/policy.html#account-ownership-password-requirements-and-mfa-policy","title":"Account Ownership, Password Requirements, and MFA Policy","text":"<p>Each user is issued a unique username secured with both a password. The credentials are strictly confidential and must not be shared.</p> <ul> <li>Passwords must be strong, rotated regularly, and follow the guidelines outlined on the Passwords page.</li> <li>MFA credentials are tied to the individual user and must be securely stored.</li> <li>Shared account usage is strictly prohibited. REPACSS enforces a one-user-per-account policy without exception.</li> </ul> <p>Account Misuse Policy</p> <p>If unauthorized access or account sharing is detected, all affected accounts will be disabled immediately. Reinstatement requires a written explanation and authorization from the Principal Investigator (PI) or project lead.</p>"},{"location":"account/policy.html#security-incidents-and-reporting","title":"Security Incidents and Reporting","text":"<p>Users who suspect account compromise, unauthorized activity, or any security incident must report it immediately to the system administrators.</p> <ul> <li>Email: repacss.support@ttu.edu</li> </ul> <p>When reporting, please include as much detail as possible (e.g., timestamps, logs, screenshots) to expedite investigation and remediation.</p>"},{"location":"account/policy.html#account-lifecycle-management","title":"Account Lifecycle Management","text":"<p>REPACSS accounts follow a lifecycle process aligned with institutional research allocations.</p>"},{"location":"account/policy.html#account-provisioning","title":"Account Provisioning","text":"<ul> <li>Access is granted upon official request and must be tied to a recognized TTU research project.</li> <li>New users undergo a vetting process by the REPACSS administrative team.</li> <li>Account setup includes MFA enrollment and initial password configuration.</li> </ul>"},{"location":"account/policy.html#account-maintenance","title":"Account Maintenance","text":"<ul> <li>Accounts remain active only while associated with an ongoing research allocation.</li> <li>Users may be prompted to acknowledge and re-accept updated policy or conduct agreements.</li> </ul>"},{"location":"account/policy.html#account-deactivation","title":"Account Deactivation","text":"<p>Accounts are subject to deactivation under the following conditions:</p> Condition Action Taken End of research project Login disabled; data access granted for 60 days User-initiated request Immediate deactivation User removed by project PI Immediate deactivation Expiration of project membership Immediate deactivation Refusal to accept policy updates Access suspended until acknowledgment is received Violation of system security policies Immediate deactivation; subject to further review <p>Upon deactivation: - Login credentials and MFA tokens are invalidated. - Data may be accessible through designated transfer endpoints for a limited duration, subject to administrative approval.</p>"},{"location":"account/policy.html#proper-acknowledgment-of-repacss-resources","title":"Proper Acknowledgment of REPACSS Resources","text":"<p>Citation Requirement</p> <p>All publications, presentations, or research outputs that utilize REPACSS computing resources must include the official acknowledgment statement below.</p>"},{"location":"account/policy.html#acknowledgment-text","title":"Acknowledgment Text","text":"<p>This research used resources of the REPACSS high-performance computing system at Texas Tech University, supported in part by the National Science Foundation under NSF Award No. 2404438 and Texas Tech\u2019s High-Performance Computing Center (HPCC).</p> <p>Proper citation ensures continued support and funding for REPACSS and reflects the scientific value of computational infrastructure.</p> <p>Not Sure If This Applies?</p> <p>If your research made use of REPACSS for simulations, analysis, training, or testing, it qualifies for acknowledgment.</p>"},{"location":"account/ttu.html","title":"Texas Tech University (TTU)","text":"<p>Section Purpose</p> <p>This section outlines Texas Tech University\u2019s role in hosting and supporting the REPACSS computing platform, including details on network access, research engagement, and local technical support.</p>"},{"location":"account/ttu.html#on-campus-and-remote-access","title":"On-Campus and Remote Access","text":"<p>The REPACSS system is physically hosted within TTU's research computing infrastructure and may be accessed via the following methods:</p> <ul> <li>On Campus: Users may connect through wired Ethernet or the TTUnet Wi-Fi network.</li> <li>Off Campus: Access is available through the TTU GlobalProtect Virtual Private Network (VPN). Instructions are available on the VPN Setup Guide.</li> <li>Authentication: All system access requires secure login via SSH or authorized web-based interfaces.</li> </ul> <p>VPN Requirement</p> <p>All users must be connected to the Texas Tech University network to access REPACSS resources. This requirement applies both on and off campus.  </p> <p>Note: Users located within the Computer Science Department building may experience restricted access when using TTUnet Wi-Fi. In such cases, a VPN connection is required to ensure system accessibility.</p>"},{"location":"account/ttu.html#local-support-team","title":"Local Support Team","text":"<p>Users requiring technical assistance or consultation may contact the local support team:</p> <ul> <li>Email: repacss.support@ttu.edu</li> <li>Location: Engineering Center (EC), Texas Tech University</li> <li>Availability: Office hours by appointment \u2014 please email to schedule</li> </ul> <p>For TTU Students</p> <p>Ensure that you are logged in using your official <code>@ttu.edu</code> account when accessing internal services or participating in REPACSS-hosted events.</p>"},{"location":"connecting/index.html","title":"Establishing Connections to REPACSS","text":"<p>This document provides detailed instructions on establishing secure remote connections to the REPACSS high-performance computing system at Texas Tech University.</p>"},{"location":"connecting/index.html#system-access-requirements","title":"System Access Requirements","text":"<p>Before attempting to connect to the REPACSS environment, users must confirm the following prerequisites:</p> <ul> <li>A valid and active TTU eRaider account</li> <li>A properly configured password and Multi-Factor Authentication (MFA)</li> <li>A secure connection to the Texas Tech University (TTU) network, either via wired Ethernet or the GlobalProtect VPN</li> </ul>"},{"location":"connecting/index.html#ttunet-vpn-requirement","title":"TTUnet &amp; VPN Requirement","text":"<p>Access to REPACSS is restricted to users connected to the TTUnet network. Off-campus users, and users in buildings with known network limitations (e.g., the Computer Science Department), must use the GlobalProtect VPN.</p> <p>For assistance configuring VPN access, refer to the following guide:</p> <ul> <li>TTU GlobalProtect VPN Setup Guide</li> <li>For further assistance please contact IT Help Central</li> </ul> <p>VPN Required</p> <p>On Campus:  - You must be connected to TTUnet, either via wired Ethernet or the TTUnet Wi-Fi network.   - Other networks on campus (such as TTUguest or EduRoam) are not supported for direct access. Off Campus:  - If you are connecting from any other network, including TTUguest, EduRoam, or external internet connections, you must use the TTU GlobalProtect Virtual Private Network (VPN).  - Instructions are available on the VPN Setup Guide.  Authentication: All system access requires secure login via SSH or authorized web-based interfaces. Note: Users located within the Computer Science Department building may experience restricted access when using TTUnet Wi-Fi. If you encounter connectivity issues, connect via wired Ethernet or enable the VPN to ensure uninterrupted access.</p>"},{"location":"connecting/index.html#connecting-via-secure-shell-ssh","title":"Connecting via Secure Shell (SSH)","text":""},{"location":"connecting/index.html#instructions-for-windows-users","title":"Instructions for Windows Users","text":"<p>For users operating Microsoft Windows, it is recommended to install and configure MobaXterm:</p> <ol> <li>Download the installer from https://mobaxterm.mobatek.net</li> <li>Launch MobaXterm and create a new SSH session with the following details:</li> <li>Remote Host: <code>repacss.ttu.edu</code></li> <li>Username: Your TTU eRaider username</li> <li>Save and initiate the connection</li> </ol>"},{"location":"connecting/index.html#instructions-for-macos-and-linux-users","title":"Instructions for macOS and Linux Users","text":"<p>Users operating macOS or Linux systems can connect via the built-in terminal application:</p> <pre><code>ssh &lt;your_eRaider_username&gt;@repacss.ttu.edu\n</code></pre> <p>Upon successful authentication, the system will display a standard login banner.</p>"},{"location":"connecting/index.html#troubleshooting-ssh-connections","title":"Troubleshooting SSH Connections","text":""},{"location":"connecting/index.html#authentication-errors","title":"Authentication Errors","text":"<p>Users may encounter \"Permission denied\" or \"Too many authentication failures\" messages due to the following:</p> <ul> <li>Incorrect credentials (eRaider username or password)</li> <li>VPN not active or improperly configured</li> <li>SSH client sending too many private keys</li> </ul>"},{"location":"connecting/index.html#host-key-verification-failed","title":"Host Key Verification Failed","text":"<p>If you receive a warning that the remote host identification has changed:</p> <ol> <li>Open the file <code>~/.ssh/known_hosts</code></li> <li>Delete the line containing <code>repacss.ttu.edu</code></li> <li>Reconnect to the system and manually verify the new host fingerprint when prompted</li> </ol>"},{"location":"connecting/index.html#additional-assistance","title":"Additional Assistance","text":"<p>If all configuration steps have been completed and you are still unable to establish a connection, contact the system administrators. Be prepared to provide diagnostic output for further review:</p> <ul> <li>Email: repacss.support@ttu.edu</li> <li>Diagnostic Output: Include the result of running the SSH command with the verbose flag:   <code>bash   ssh -vvv &lt;your_eRaider_username&gt;@repacss.ttu.edu</code></li> </ul>"},{"location":"connecting/mfa.html","title":"MFA Setup","text":""},{"location":"connecting/mfa.html#configuring-microsoft-multi-factor-authentication-mfa","title":"Configuring Microsoft Multi-Factor Authentication (MFA)","text":"<p>Access to REPACSS requires that all users have Microsoft Multi-Factor Authentication (MFA) enabled on their TTU eRaider accounts.</p> <p>To configure or manage your MFA settings, please visit the official TTU support portal:</p> <p>Configure Microsoft MFA at Texas Tech University</p> <p>Important</p> <p>Multi-Factor Authentication is mandatory for authenticating through the TTU GlobalProtect VPN. Users must complete MFA setup prior to attempting to connect to REPACSS.</p>"},{"location":"connecting/mfa.html#support","title":"Support","text":"<p>Users experiencing difficulties with MFA enrollment or authentication should contact TTU IT Help Central:</p> <ul> <li>Email: ithelpcentral@ttu.edu </li> <li>Phone: 806-742-HELP (4357)  </li> <li>Web: IT Help Central</li> </ul>"},{"location":"connecting/vpn.html","title":"VPN Setup Guide for REPACSS","text":"<p>This document outlines the procedure for configuring and utilizing the GlobalProtect VPN client to securely access the REPACSS high-performance computing (HPC) infrastructure at Texas Tech University (TTU).</p>"},{"location":"connecting/vpn.html#overview","title":"Overview","text":"<p>All users must establish a secure network connection via TTU\u2019s GlobalProtect VPN to access REPACSS (<code>repacss.ttu.edu</code>). VPN access requires:</p> <ul> <li>Use of the portal address <code>vpn.ttu.edu</code></li> <li>Authentication through a valid TTU eRaider account</li> <li>An active Microsoft Multi-Factor Authentication (MFA) configuration</li> </ul> <p>This guide provides setup instructions for the following operating systems:</p> <ul> <li>Microsoft Windows</li> <li>Apple macOS</li> <li>GNU/Linux distributions</li> </ul>"},{"location":"connecting/vpn.html#step-1-request-vpn-access","title":"Step 1: Request VPN Access","text":"<p>Prior to installing the VPN client, users must formally request access:</p> <ol> <li>Navigate to the TTU VPN Access Request Form and make sure you are signed-in to see the form.</li> <li>Under \"Type of Assistance,\" select: <code>Enable</code></li> <li>Provide the following justification:    <pre><code>Need TTUnet VPN to access REPACSS TTU Cluster\n</code></pre></li> <li>Confirm that Microsoft MFA is active and functional</li> </ol>"},{"location":"connecting/vpn.html#windows-configuration","title":"Windows Configuration","text":""},{"location":"connecting/vpn.html#installation","title":"Installation","text":"<ol> <li>Visit the TTU Software Distribution Portal</li> <li>Download the appropriate <code>.msi</code> file (32-bit or 64-bit)</li> <li>Execute the installer and follow the on-screen instructions</li> <li>Reboot if prompted</li> </ol> <p>For more info please see IT Help Central's official document.</p>"},{"location":"connecting/vpn.html#vpn-connection-procedure","title":"VPN Connection Procedure","text":"<ol> <li>Launch the GlobalProtect application</li> <li>Enter the portal address:    <pre><code>vpn.ttu.edu\n</code></pre></li> <li>Authenticate using TTU eRaider credentials and approve the MFA prompt</li> <li>Upon successful connection, status will display as Connected</li> </ol>"},{"location":"connecting/vpn.html#macos-configuration","title":"macOS Configuration","text":""},{"location":"connecting/vpn.html#installation_1","title":"Installation","text":"<ol> <li>Download the GlobalProtect <code>.pkg</code> installer from TTU Software Distribution Portal</li> <li>Launch the installer and complete all prompts</li> <li>Grant required permissions if prompted by the operating system</li> </ol>"},{"location":"connecting/vpn.html#vpn-connection-procedure_1","title":"VPN Connection Procedure","text":"<ol> <li>Open GlobalProtect from the menu bar</li> <li>Enter the portal:    <pre><code>vpn.ttu.edu\n</code></pre></li> <li>Authenticate using TTU eRaider credentials and Microsoft MFA</li> <li>Confirm connection status in the application interface   </li> </ol> <p>For more info please see IT Help Central's official document.</p>"},{"location":"connecting/vpn.html#linux-configuration","title":"Linux Configuration","text":""},{"location":"connecting/vpn.html#prerequisites-ubuntudebian","title":"Prerequisites (Ubuntu/Debian)","text":"<p>Install system dependencies:</p> <pre><code>sudo apt-get update\nsudo apt-get install libc6 libstdc++6 libpam0g libx11-6 libxcb1 \\\nlibxcomposite1 libxcursor1 libxdamage1 libxext6 libxfixes3 libxi6 \\\nlibxrandr2 libxrender1 libxtst6 libnss3 libgtk-3-0\n</code></pre>"},{"location":"connecting/vpn.html#installation_2","title":"Installation","text":"<ol> <li>Visit TTU Software Distribution Portal</li> <li>Download the appropriate <code>.deb</code> or <code>.rpm</code> package</li> </ol> <p>Install the package:</p> <p>Debian/Ubuntu <pre><code>sudo dpkg -i GlobalProtect_&lt;version&gt;_amd64.deb\n</code></pre></p> <p>RHEL/CentOS <pre><code>sudo rpm -i GlobalProtect_&lt;version&gt;_x86_64.rpm\n</code></pre></p> <p>For more info please see IT Help Central's official document.</p>"},{"location":"connecting/vpn.html#vpn-connection-procedure_2","title":"VPN Connection Procedure","text":"<p>Launch the client UI:</p> <pre><code>globalprotect launch-ui\n</code></pre> <ol> <li>Input portal:    <pre><code>vpn.ttu.edu\n</code></pre></li> <li>Authenticate using your TTU eRaider credentials and MFA</li> </ol> <p>Verify connection status:</p> <pre><code>globalprotect show --status\n</code></pre> <p>Expected output: <pre><code>Status: Connected\n</code></pre></p>"},{"location":"connecting/vpn.html#disconnecting-from-vpn","title":"Disconnecting from VPN","text":"<p>Windows/macOS: Click the GlobalProtect icon and select Disconnect</p> <p>Linux:</p> <pre><code>globalprotect disconnect\n</code></pre>"},{"location":"connecting/vpn.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"connecting/vpn.html#authentication-issues","title":"Authentication Issues","text":"<ul> <li>Verify username and MFA credentials</li> <li>Attempt login in a private browser session</li> <li>Clear stored cookies and cached credentials</li> </ul>"},{"location":"connecting/vpn.html#connection-errors","title":"Connection Errors","text":"<ul> <li>Confirm the portal address is correct: <code>vpn.ttu.edu</code></li> <li>Restart GlobalProtect and your device</li> <li>Ensure firewall settings permit VPN traffic</li> </ul>"},{"location":"connecting/vpn.html#installation-failures","title":"Installation Failures","text":"<ul> <li>Ensure administrative privileges are granted</li> <li>Validate that system requirements are met</li> <li>Re-download installer from official TTU source</li> </ul>"},{"location":"connecting/vpn.html#additional-resources","title":"Additional Resources","text":"<ul> <li>Getting Started at REPACSS</li> <li>Running Jobs on REPACSS</li> </ul>"},{"location":"connecting/vscode.html","title":"Visual Studio Code","text":"<p>Visual Studio Code is a modern, lightweight IDE (Integrated Development Environment) that supports remote development through SSH. It is highly customizable and offers a wide range of extensions for languages and workflows common in HPC environments, such as Python, C/C++, Fortran, and more.</p> <p>Warning</p> <p>When using the integrated terminal in Visual Studio Code to connect to the system, the <code>interactive</code> alias may not function as intended. This can lead to unexpected behavior when attempting to request compute resources.</p> <p>Success</p> <p>Users should open a standard terminal session and run the <code>interactive</code> command there to ensure proper configuration of the interactive session.</p>"},{"location":"connecting/vscode.html#connecting-with-vscode-remote-ssh","title":"Connecting with VSCode Remote SSH","text":"<p>Connecting to REPACSS via the Remote - SSH extension allows you to edit files, run terminals, and debug code directly on the system \u2014 just like you would on your local machine.</p> <p>By default, VSCode will connect you to a REPACSS login node, which is suitable for lightweight tasks like editing code or running small programs. For heavy workloads, you should request access to a compute node using Slurm.</p> <p>Warning</p> <p>If your home directory on REPACSS exceeds its quota, VSCode Remote SSH connections may silently fail. Be sure to check your usage and offload files if needed.</p>"},{"location":"connecting/vscode.html#ssh-configuration-for-repacss","title":"SSH Configuration for REPACSS","text":"<p>To streamline your VSCode connection, add the following to your <code>~/.ssh/config</code> file on your local machine:</p> <pre><code># Login node configuration\nHost repacss\n    HostName repacss.hpcc.ttu.edu\n    User your_ttu_username\n    IdentityFile ~/.ssh/repacss\n    IdentitiesOnly yes\n    ForwardAgent yes\n    LogLevel QUIET\n</code></pre> <p>If you're on Windows, you can add the same block in: <pre><code>C:\\Users\\YourUsername\\.ssh\\config\n</code></pre></p> <p>Make sure your SSH key is added to your ssh-agent and that your eRaider account has VPN and MFA enabled (see VPN Setup and MFA Setup).</p>"},{"location":"performance/compiler-flags.html","title":"Compiler Flags","text":"<p>Warning</p> <p>This page is currently under development. The following recommendations serve as a preliminary guide. Users are strongly encouraged to validate compiler flags against their own applications through testing and performance benchmarking.</p> <p>Optimizing compilation flags is essential to maximizing application performance on the REPACSS high-performance computing system. The recommended flags below are tailored for the compiler toolchains available via REPACSS environment modules.</p>"},{"location":"performance/compiler-flags.html#general-optimization-guidelines","title":"General Optimization Guidelines","text":"<ul> <li>Use <code>-O2</code> or <code>-O3</code> for performance optimization.</li> <li>Enable vectorization and loop unrolling where beneficial.</li> <li>Use architecture-specific flags (e.g., <code>-march=native</code>) to generate code optimized for REPACSS hardware.</li> <li>Profile and benchmark performance before and after applying new flags.</li> <li>Use <code>-fopenmp</code> to enable multithreading where applicable.</li> </ul>"},{"location":"performance/compiler-flags.html#gcc-version-1420","title":"GCC (Version 14.2.0)","text":"<p>Recommended Flags:</p> <pre><code>-O3 -march=native -funroll-loops -ffast-math -fopenmp\n</code></pre> <p>Descriptions:</p> <ul> <li><code>-O3</code>: Enables aggressive optimization.</li> <li><code>-march=native</code>: Targets the specific microarchitecture of the compute node.</li> <li><code>-funroll-loops</code>: Enhances performance for loop-heavy code.</li> <li><code>-ffast-math</code>: Permits faster floating-point operations (may break strict IEEE compliance).</li> <li><code>-fopenmp</code>: Enables OpenMP multithreading.</li> </ul>"},{"location":"performance/compiler-flags.html#example-compilation-command","title":"Example Compilation Command","text":"<p>GCC Example:</p> <pre><code>gcc -O3 -march=native -fopenmp mycode.c -o mycode\n</code></pre> <p>Note</p> <p>For best results, benchmark with realistic datasets and verify correctness after applying aggressive optimizations.</p>"},{"location":"performance/profiling-tools.html","title":"Profiling Tools","text":"<p>Warning</p> <p>This page is under development. Details about performance analysis tools will be added soon.</p> <p>We are actively working to provide detailed guidance on profiling tools available in REPACSS, including how to use tools like <code>gprof</code>, <code>perf</code>, Intel VTune, NVIDIA Nsight, and others.</p> <p>Please check back soon!</p>"},{"location":"performance/scaling-tests.html","title":"Scaling Tests","text":"<p>Warning</p> <p>This page is under development. Examples and benchmarks for strong/weak scaling will be provided soon.</p> <p>We are preparing a comprehensive guide on how to perform strong and weak scaling tests on REPACSS, along with recommendations for benchmarking and interpreting results.</p> <p>Please check back later for updates!</p>"},{"location":"reference/cheatsheet.html","title":"REPACSS Command Cheatsheet","text":"<p>Quick reference for commonly used commands and options on REPACSS.</p>"},{"location":"reference/cheatsheet.html#job-submission","title":"Job Submission","text":"<pre><code>sbatch job.sh                    # Submit a batch job\ninteractive -c 8 -p zen4         # Start an interactive session (recommended)\nsrun ./program                   # Run a job step\n</code></pre> <p>Tip</p> <p>Always use the <code>interactive</code> command for starting real-time sessions. It properly configures the environment and avoids common issues that arise when using <code>salloc</code> directly.</p>"},{"location":"reference/cheatsheet.html#monitoring-jobs","title":"Monitoring Jobs","text":"<pre><code>squeue -u $USER                  # Show your jobs\nsqs                              # Enhanced view of job queue\nscontrol show job &lt;jobid&gt;       # Job details\n</code></pre>"},{"location":"reference/cheatsheet.html#cancel-jobs","title":"Cancel Jobs","text":"<pre><code>scancel &lt;jobid&gt;                  # Cancel specific job\nscancel -u $USER                 # Cancel all your jobs\n</code></pre>"},{"location":"reference/cheatsheet.html#modules","title":"Modules","text":"<pre><code>module avail                     # List available modules\nmodule load gcc/12.2.0           # Load specific module\nmodule list                      # Show loaded modules\nmodule unload gcc/12.2.0         # Unload module\n</code></pre>"},{"location":"reference/cheatsheet.html#software-setup","title":"Software Setup","text":"<pre><code>conda activate myenv             # Activate Python environment\npip install mypackage            # Install Python package\nspack install hdf5               # Install with Spack\n</code></pre>"},{"location":"reference/cheatsheet.html#file-management","title":"File Management","text":"<pre><code>ls -lh                           # List files with size\ndu -sh *                         # Check folder sizes\ndf -h                            # Check disk usage\n</code></pre>"},{"location":"reference/cheatsheet.html#environment-variables","title":"Environment Variables","text":"<pre><code>export OMP_NUM_THREADS=8         # Set OpenMP threads\nexport SBATCH_ACCOUNT=m1234      # Set default Slurm account\n</code></pre>"},{"location":"reference/cheatsheet.html#performance","title":"Performance","text":"<pre><code>time ./program                   # Quick runtime check\nperf stat ./program              # Performance statistics (Linux)\n</code></pre>"},{"location":"reference/cheatsheet.html#help-and-docs","title":"Help and Docs","text":"<pre><code>man sbatch                       # Manual for sbatch\nsqueue --help                    # Help for squeue\nmodule help gcc                  # Help for module\n</code></pre>"},{"location":"reference/common-errors.html","title":"Troubleshooting Guide","text":"<p>Warning</p> <p>This page is still under development. The content below may be incomplete or subject to revision.</p> <p>Use this page to identify and resolve common issues encountered on REPACSS.</p>"},{"location":"reference/common-errors.html#job-not-starting","title":"Job Not Starting","text":"<ul> <li>Check if your job is pending due to resource availability:</li> </ul> <pre><code>squeue --me\n</code></pre> <ul> <li>Look for the NODELIST(REASON) column for hints like:</li> <li><code>Resources</code>: Not enough nodes available.</li> <li><code>Priority</code>: Waiting for higher-priority jobs to clear.</li> <li><code>Dependency</code>: Job depends on another that hasn\u2019t finished.</li> </ul>"},{"location":"reference/common-errors.html#invalid-account-or-partition","title":"Invalid Account or Partition","text":"<p>If you see: <pre><code>sbatch: error: Job request does not match any supported policy.\n</code></pre></p> <ul> <li>Ensure your account/project is valid and available for the selected partition.</li> <li>Set your account using:</li> </ul> <pre><code>#SBATCH --account=m1234\n</code></pre>"},{"location":"reference/common-errors.html#python-module-not-found","title":"Python Module Not Found","text":"<p>If you get: <pre><code>ModuleNotFoundError: No module named 'numpy'\n</code></pre></p> <ul> <li> <p>Check that you\u2019ve activated the correct conda environment:   <pre><code>conda activate myenv\n</code></pre></p> </li> <li> <p>If it\u2019s missing, reinstall it:   <pre><code>pip install numpy\n</code></pre></p> </li> </ul>"},{"location":"reference/common-errors.html#ssh-connection-issues","title":"SSH Connection Issues","text":"<ul> <li>If login is slow or stuck:</li> <li> <p>Try verbose output to debug:     <pre><code>ssh -vvv your_username@repacss.ttu.edu\n</code></pre></p> </li> <li> <p>Make sure VPN is connected properly.</p> </li> </ul>"},{"location":"reference/common-errors.html#command-not-found","title":"\"command not found\"","text":"<p>If a common command like <code>gcc</code>, <code>python</code>, or <code>nvcc</code> fails: - Check if the required module is loaded:   <pre><code>module list\nmodule load gcc/12.2.0\n</code></pre></p>"},{"location":"reference/common-errors.html#poor-performance","title":"Poor Performance","text":"<ul> <li>Check if you\u2019re using correct compiler flags and parallel settings.</li> <li> <p>Use profiling tools like:   <pre><code>time ./a.out\nperf stat ./a.out\n</code></pre></p> </li> <li> <p>Review CPU or GPU utilization via <code>sstat</code> or <code>jobstats</code>.</p> </li> </ul>"},{"location":"reference/common-errors.html#disk-quota-or-storage-issues","title":"Disk Quota or Storage Issues","text":"<p>If you see: <pre><code>Disk quota exceeded\n</code></pre></p> <ul> <li>Use <code>du -sh *</code> to identify large files.</li> <li>Remove or move unnecessary data.</li> </ul>"},{"location":"reference/common-errors.html#data-transfer-fails","title":"Data Transfer Fails","text":"<ul> <li>If <code>scp</code> or <code>rsync</code> fails:</li> <li>Double-check path and permissions.</li> <li>For large transfers, consider:     <pre><code>rsync -avzP source/ user@repacss.ttu.edu:/destination/\n</code></pre></li> </ul>"},{"location":"reference/common-errors.html#infinite-loop-or-hanging-job","title":"Infinite Loop or Hanging Job","text":"<ul> <li>Make sure loops have valid exit conditions.</li> <li>Add logging or print statements to verify execution flow.</li> <li>You can cancel the job with:   <pre><code>scancel &lt;jobid&gt;\n</code></pre></li> </ul>"},{"location":"reference/common-errors.html#module-conflicts","title":"Module Conflicts","text":"<ul> <li>If loading a module breaks something:   <pre><code>module purge\nmodule load only_what_you_need\n</code></pre></li> </ul>"},{"location":"reference/common-errors.html#need-help","title":"Need Help?","text":"<p>If all else fails:</p> <ul> <li>Reach out to your advisor or system admin.</li> <li>Provide job ID, error message, and the job script when asking for help.</li> </ul>"},{"location":"reference/glossary.html","title":"Glossary","text":"<p>This section defines key terms frequently used throughout the REPACSS documentation.</p>"},{"location":"reference/glossary.html#a","title":"A","text":"<p>Account A project allocation or funding unit used to charge for computing time.</p> <p>Allocation The computing time assigned to a project, typically measured in node-hours.</p>"},{"location":"reference/glossary.html#b","title":"B","text":"<p>Batch Job A job submitted to run without user interaction, scheduled by the system.</p>"},{"location":"reference/glossary.html#c","title":"C","text":"<p>Compute Node A server node that runs user jobs and provides computational power.</p> <p>Container A lightweight, portable software environment used to ensure consistent runtime.</p>"},{"location":"reference/glossary.html#g","title":"G","text":"<p>GPU (Graphics Processing Unit) A specialized processor optimized for parallel processing, commonly used in AI and simulations.</p>"},{"location":"reference/glossary.html#i","title":"I","text":"<p>Interactive Job A job that allows users to run commands in real time on compute nodes.</p>"},{"location":"reference/glossary.html#m","title":"M","text":"<p>Module System Environment management system used to load software dynamically.</p>"},{"location":"reference/glossary.html#p","title":"P","text":"<p>Partition A subset of nodes within the cluster grouped together for scheduling (e.g., <code>zen4</code>, <code>h100</code>).</p>"},{"location":"reference/glossary.html#s","title":"S","text":"<p>Slurm The workload manager used by REPACSS to schedule and monitor jobs.</p> <p>srun A command to launch parallel tasks under Slurm.</p> <p>salloc Allocates resources interactively under Slurm.</p> <p>sbatch Submits batch job scripts to the scheduler.</p>"},{"location":"running-jobs/basics.html","title":"Basics of Running Jobs","text":"<p>REPACSS utilizes the Slurm Workload Manager for resource allocation and job scheduling across its high-performance computing infrastructure. Slurm manages the assignment of compute resources, oversees execution and monitoring, and enables the scheduling of tasks for future execution.</p>"},{"location":"running-jobs/basics.html#additional-resources","title":"Additional Resources","text":"<ul> <li>Slurm Documentation</li> <li>Slurm Tutorials</li> <li>Slurm Manual Pages</li> <li>Slurm FAQ</li> </ul>"},{"location":"running-jobs/basics.html#job-definition","title":"Job Definition","text":"<p>A job is defined as an allocation of compute resources granted to a user for a specified duration. Jobs may be executed interactively or as batch processes (via job scripts) and can be scheduled to run at a future time.</p> <p>Note</p> <p>REPACSS provides sample job submission scripts and templates for common workloads.</p> <p>Upon accessing REPACSS, users arrive at a login node, which is intended for job preparation activities such as file editing or code compilation. All computational jobs must be submitted to compute nodes using Slurm commands.</p> <p>The platform supports a wide range of workflows including interactive sessions, serial executions, GPU-based applications, and large-scale parallel computations.</p>"},{"location":"running-jobs/basics.html#submitting-jobs","title":"Submitting Jobs","text":""},{"location":"running-jobs/basics.html#sbatch","title":"<code>sbatch</code>","text":"<p>Submit a batch script:</p> <pre><code>$ sbatch my_job.sh\nSubmitted batch job 864933\n</code></pre> <p>Job scripts should include <code>#SBATCH</code> directives and one or more <code>srun</code> commands.</p>"},{"location":"running-jobs/basics.html#interactive","title":"<code>interactive</code>","text":"<p>Request an interactive session using the recommended wrapper script:</p> <pre><code>$ interactive -c 8 -p zen4\n</code></pre> <p>Tip</p> <p>The <code>interactive</code> command wraps around Slurm's allocation mechanisms and handles additional environment setup required for compute node access. Avoid using <code>salloc</code> directly \u2014 especially in environments like Visual Studio Code \u2014 as it may fail to configure session parameters properly.</p> <p>An interactive shell will be initiated on a compute node once resources are allocated.</p>"},{"location":"running-jobs/basics.html#srun","title":"<code>srun</code>","text":"<p>Execute a job step in real-time:</p> <pre><code>$ srun -n 4 ./program\n</code></pre> <p>May be used within job scripts or interactive sessions.</p>"},{"location":"running-jobs/basics.html#commonly-used-options","title":"Commonly Used Options","text":"Option (long) Short Description sbatch srun <code>--time</code> <code>-t</code> Maximum wall clock time \u2705 \u274c <code>--nodes</code> <code>-N</code> Number of nodes \u2705 \u2705 <code>--ntasks</code> <code>-n</code> Number of parallel tasks (e.g., MPI) \u2705 \u2705 <code>--cpus-per-task</code> <code>-c</code> CPU cores allocated per task \u2705 \u2705 <code>--gpus</code> <code>-G</code> Number of GPUs requested \u2705 \u2705 <code>--constraint</code> <code>-C</code> Specific hardware or node type constraint \u2705 \u274c <code>--qos</code> <code>-q</code> Quality of Service tier \u2705 \u274c <code>--account</code> <code>-A</code> Project account for usage tracking \u2705 \u274c <code>--job-name</code> <code>-J</code> Name assigned to the job \u2705 \u274c <p>Tip</p> <p>It is advisable to use long-form flags (e.g., <code>--nodes=2</code>) in scripts for clarity and maintainability.</p>"},{"location":"running-jobs/basics.html#writing-a-job-script","title":"Writing a Job Script","text":"<p>Sample job script:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=test\n#SBATCH --nodes=2\n#SBATCH --time=01:00:00\n#SBATCH --partition=h100\n#SBATCH --account=mXXXX\n\nmodule load gcc\n\nsrun -n 4 ./a.out\n</code></pre> <p>Slurm options may also be specified at the command line:</p> <pre><code>sbatch -N 2 -p h100 ./job.sh\n</code></pre>"},{"location":"running-jobs/basics.html#option-inheritance-and-overriding","title":"Option Inheritance and Overriding","text":"<p>Slurm options may be declared within the job script via <code>#SBATCH</code> or specified directly on the command line. If both are present, command line options take precedence.</p> <p>Environment variables such as <code>SLURM_JOB_NUM_NODES</code> are automatically populated and propagated to <code>srun</code>. Avoid redundant overrides within the script unless necessary. Behavior may vary when using non-Slurm launch mechanisms.</p>"},{"location":"running-jobs/basics.html#sample-python-job-script","title":"Sample Python Job Script","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=python_job\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=32G\n#SBATCH --time=01:00:00\n#SBATCH --partition=zen4\n\nmodule load gcc\nsource ~/miniforge3/etc/profile.d/conda.sh\nconda activate myenv\n\npython script.py\n</code></pre>"},{"location":"running-jobs/basics.html#submitting-gpu-jobs","title":"Submitting GPU Jobs","text":"<p>Warning</p> <p>We are currently in the process of installing the CUDA module on our systems. In the meantime, users are advised to install CUDA via their conda environment to ensure compatibility with GPU workflows.  For detailed usage examples, please refer to the Job Examples section.</p> <p>To request GPU resources, use the <code>--gres</code> flag:</p> <pre><code>#SBATCH --gres=gpu:nvidia_h100_nvl:1\n</code></pre> <p>Ensure required modules such as CUDA are loaded:</p> <pre><code>module load cuda\n</code></pre> <p>Failure to request GPUs may result in errors such as:</p> <pre><code>No CUDA-capable device is detected\n</code></pre>"},{"location":"running-jobs/basics.html#job-monitoring","title":"Job Monitoring","text":"<p>Check job queue status:</p> <pre><code>squeue -u $USER\n</code></pre> <p>Estimate job start time:</p> <pre><code>squeue --start -j &lt;job_id&gt;\n</code></pre> <p>Review completed job statistics:</p> <pre><code>sacct -j &lt;job_id&gt;\n</code></pre>"},{"location":"running-jobs/basics.html#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Verify group disk quotas: <code>df -h /mnt/$(id -gn)</code></li> <li>Ensure job script includes required Slurm options</li> <li>Confirm partition and hardware constraints match available resources</li> <li>Load relevant modules before execution</li> <li>Inspect job output files (<code>.out</code>, <code>.err</code>) for detailed error messages</li> </ul>"},{"location":"running-jobs/basics.html#additional-documentation","title":"Additional Documentation","text":"<ul> <li>Job Examples</li> <li>Monitoring Jobs</li> <li>Interactive Sessions</li> <li>Best Practices</li> </ul>"},{"location":"running-jobs/best-practices.html","title":"Best Practices for REPACSS Users","text":"<p>This document provides formal guidance for effective and responsible utilization of the REPACSS computing infrastructure. Adherence to these practices ensures optimal system performance, equitable access for all users, and improved job throughput across the cluster.</p>"},{"location":"running-jobs/best-practices.html#general-usage-recommendations","title":"General Usage Recommendations","text":"<p>Users are required to specify realistic job durations using the <code>--time</code> directive. Accurately estimating wall time enables efficient scheduling and backfilling, while overestimation may delay other users\u2019 jobs. Underestimation, on the other hand, may result in the premature termination of running jobs.</p> <p>Resource requests submitted via <code>--nodes</code>, <code>--ntasks</code>, and <code>--cpus-per-task</code> must reflect the actual needs of the application. Unnecessary overprovisioning leads to inefficient use of shared computing resources and may increase queue times.</p> <p>When submitting large numbers of similar short jobs, it is advisable to use job arrays via the <code>--array</code> directive. This approach significantly reduces the overhead imposed on the job scheduler.</p> <p>To ensure fair usage, users must refrain from launching multiple monitoring commands in parallel, such as multiple instances of <code>watch squeue</code>. If use of <code>watch</code> is necessary, the polling interval should be set to 60 seconds or longer.</p> <p>Users are encouraged to automate job submission and data processing workflows through the use of scripts. This practice promotes consistency, reduces manual errors, and supports long-term reproducibility.</p> <p>Project directories should be organized in a structured manner. Output files, logs, and job scripts should be stored in logically named subdirectories to facilitate navigation and archival.</p>"},{"location":"running-jobs/best-practices.html#development-and-production-job-workflow","title":"Development and Production Job Workflow","text":"<p>Prior to submitting large-scale jobs, users must perform validation using short, small-scale test jobs. Such tests should employ reduced node counts and minimal wall time limits to identify and resolve application errors efficiently.</p> <p>After successful execution of a small-scale job, users may scale up their resource requests incrementally. This ensures that production jobs are submitted only after validating the program\u2019s correctness and performance.</p> <p>For debugging and development purposes, users are strongly encouraged to initiate interactive sessions using the <code>interactive</code> command rather than calling <code>salloc</code> directly. The <code>interactive</code> command handles additional environment configuration and resource setup critical for successful session initialization.</p> <p>A one-hour interactive session using one or two compute nodes is generally sufficient for most diagnostic tasks. For example:</p> <pre><code>interactive -c 8 -p zen4\n</code></pre> <p>Tip</p> <p>Using the <code>interactive</code> command is the officially recommended method for launching temporary, real-time compute jobs. Avoid using <code>salloc</code> directly, especially when working through environments like Visual Studio Code or remote shells, as this may bypass necessary setup steps.</p> <p>Upon completion of any job, users should assess system utilization using tools such as <code>sacct</code>, <code>sstat</code>, or <code>jobstats</code>. This post-job analysis enables performance tuning and resource optimization.</p>"},{"location":"running-jobs/best-practices.html#job-submission-and-scheduling-conduct","title":"Job Submission and Scheduling Conduct","text":"<p>Repeated cancellation and resubmission of jobs is discouraged. Doing so interferes with the efficiency of the job scheduler and may negatively impact the scheduling of other users\u2019 jobs.</p> <p>In cases where a job should be temporarily paused, the recommended procedure is to place the job on hold using the <code>scontrol hold</code> command. Once the job is ready for execution, it can be released using <code>scontrol release</code>.</p> <p>Users must cancel any jobs that are no longer required or that have remained in the pending state for extended periods without justification. The <code>scancel</code> command should be used to remove such jobs from the queue promptly.</p>"},{"location":"running-jobs/best-practices.html#monitoring-and-diagnostic-tools","title":"Monitoring and Diagnostic Tools","text":"<p>During job execution, users may retrieve diagnostic information by executing <code>scontrol show job &lt;jobid&gt;</code> and identifying the compute nodes assigned to the job. Secure shell (SSH) access to these nodes is permitted only while the job is actively running.</p> <p>Monitoring tools such as <code>squeue</code>, <code>sacct</code>, and <code>jobstats</code> are available for tracking job states, resource utilization, and job history. When using the <code>watch</code> utility for monitoring, users must ensure that the polling interval does not fall below 60 seconds to minimize scheduler impact.</p> <p>To receive email notifications on job state changes, users may include the following SLURM directives in their job submission scripts:</p> <pre><code>#SBATCH --mail-type=begin,end,fail\n#SBATCH --mail-user=you@example.com\n</code></pre>"},{"location":"running-jobs/best-practices.html#file-management-and-data-storage","title":"File Management and Data Storage","text":"<p>Upon job completion, users are expected to delete all temporary files, logs, and intermediate data that are no longer needed. This ensures sufficient storage space is available for all users.</p> <p>Users should regularly archive output data to long-term storage systems or transfer it to local infrastructure. Large result sets should not remain on cluster storage indefinitely.</p> <p>To avoid exceeding storage quotas, users must monitor disk usage and maintain a manageable project footprint. File compression and proper archiving practices are encouraged.</p>"},{"location":"running-jobs/best-practices.html#documentation-and-research-reproducibility","title":"Documentation and Research Reproducibility","text":"<p>Users must document all job parameters, including SBATCH directives, environment configurations, input datasets, and output expectations. These records are critical for reproducing results and auditing scientific workflows.</p> <p>Version control systems such as Git should be used to track source code, job scripts, and configuration files. Committing all components of a research workflow ensures consistency and traceability.</p> <p>Each project directory should include a README file that describes the folder structure, data input requirements, and interpretation of output files. This is especially important in collaborative environments.</p>"},{"location":"running-jobs/best-practices.html#collaboration-and-support-channels","title":"Collaboration and Support Channels","text":"<p>In collaborative projects, shared scripts must be clearly documented and free of user-specific paths or variables. Project members should coordinate job submissions to avoid contention for resources.</p> <p>If technical issues arise, users are expected to submit a support ticket through the appropriate channels. The request should include job IDs, error logs, and relevant command history.</p> <p>All users should stay informed of REPACSS operational updates by monitoring official announcements, maintenance notifications, and policy changes issued by the system administrators.</p> <p>For further assistance, please consult the Support page.</p>"},{"location":"running-jobs/determining-resource-requirements.html","title":"Determining Resource Requirements","text":"<p>Before submitting production workloads to REPACSS, it is essential to accurately estimate the resources your jobs will require. Overestimating or underestimating resources can lead to inefficient scheduling, failed jobs, or unnecessary queuing delays. This exercise guides you through strategies to test and fine-tune resource requests in your SLURM job scripts.</p>"},{"location":"running-jobs/determining-resource-requirements.html#overview-of-slurm-resource-requests","title":"Overview of SLURM Resource Requests","text":"<p>SLURM provides several directives to declare resource needs in your job script:</p> <ul> <li><code>--ntasks</code>: Number of tasks to launch (often <code>1</code> for serial jobs).</li> <li><code>--cpus-per-task</code>: Number of CPU cores required per task.</li> <li><code>--mem</code>: Amount of memory needed per node.</li> <li><code>--time</code>: Estimated maximum runtime for the job.</li> <li><code>--gres</code>: Specification for special resources, such as GPUs.</li> </ul> <p>Explicitly specifying these parameters is recommended, especially when submitting multiple or resource-intensive jobs.</p>"},{"location":"running-jobs/determining-resource-requirements.html#consequences-of-incorrect-resource-requests","title":"Consequences of Incorrect Resource Requests","text":""},{"location":"running-jobs/determining-resource-requirements.html#underestimating-resources","title":"Underestimating Resources","text":"<p>If your job exceeds the allocated resources (for example, memory), SLURM will terminate the job. Partial results may be lost. Therefore, it is critical to request sufficient resources to avoid interruptions.</p>"},{"location":"running-jobs/determining-resource-requirements.html#overestimating-resources","title":"Overestimating Resources","text":"<p>Excessive resource requests can increase wait times, as fewer nodes have sufficient free resources to start your job. Over-requesting also limits availability for other users. For fairness and efficiency, request only what you expect to use.</p>"},{"location":"running-jobs/determining-resource-requirements.html#estimating-resource-requirements","title":"Estimating Resource Requirements","text":"<p>If you are unsure of your job\u2019s memory or disk usage, you have two main options:</p> <ul> <li>Estimate requirements in advance using local tests and monitoring.</li> <li>Run a test job with conservative resource allocations and measure actual usage.</li> </ul>"},{"location":"running-jobs/determining-resource-requirements.html#observing-memory-usage-locally","title":"Observing Memory Usage Locally","text":"<p>Warning</p> <p>Do not execute computationally intensive jobs on shared login nodes. Use your personal workstation or a designated test environment to avoid interfering with other users.</p> <p>On macOS or Windows, you can monitor processes using Activity Monitor or Task Manager. On Linux, the <code>ps</code> and <code>top</code> utilities can help track memory usage.</p> <p>Example: Using <code>ps</code></p> <pre><code>ps ux\n</code></pre> <p>The <code>RSS</code> column shows approximate memory usage in kilobytes.</p> <p>Example: Using <code>top</code></p> <pre><code>top -u &lt;username&gt;\n</code></pre> <p>The <code>RES</code> column shows memory usage in real time. Press <code>q</code> to exit.</p>"},{"location":"running-jobs/determining-resource-requirements.html#estimating-disk-usage","title":"Estimating Disk Usage","text":"<p>Disk usage includes:</p> <ul> <li>Executable binaries</li> <li>Input files transferred to the job</li> <li>Output files generated during execution</li> <li>Temporary files created by your application</li> </ul> <p>You can check file sizes on a local system with:</p> <pre><code>ls -lh\ndu -sh\n</code></pre>"},{"location":"running-jobs/determining-resource-requirements.html#determining-resource-requirements-by-running-test-jobs-recommended","title":"Determining Resource Requirements by Running Test Jobs (Recommended)","text":"<p>The most reliable method to measure your job\u2019s resource needs is to submit a small-scale test job and examine the SLURM job statistics after completion.</p> <p>Example Test Script</p> <pre><code>#!/usr/bin/env python3\nimport time\nsize = 1000000\nnumbers = [str(i) for i in range(size)]\nwith open('numbers.txt', 'w') as f:\n    f.write(' '.join(numbers))\ntime.sleep(60)\n</code></pre> <p>Example SLURM Job Script (<code>test_job.slurm</code>)</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=resource_test\n#SBATCH --output=resource_test.out\n#SBATCH --error=resource_test.err\n#SBATCH --time=00:05:00\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=1G\n\nmodule load python\n\nsrun python test_script.py\n</code></pre> <p>Submit the job:</p> <pre><code>sbatch test_job.slurm\n</code></pre> <p>When the job completes, check memory and CPU usage:</p> <pre><code>seff &lt;jobid&gt;\n</code></pre> <p>Example <code>seff</code> Output:</p> <pre><code>Job ID: 12345\nState: COMPLETED (exit code 0)\nCores: 1\nCPU Utilized: 00:01:00\nMemory Utilized: 98.50 MB\n</code></pre>"},{"location":"running-jobs/determining-resource-requirements.html#specifying-resource-requests-in-job-scripts","title":"Specifying Resource Requests in Job Scripts","text":"<p>Based on test job results, update your SLURM script with appropriate resource estimates, rounding up modestly to allow for variability:</p> <pre><code>#SBATCH --mem=120M     # Rounded up from ~99 MB\n</code></pre> <p>Important Notes:</p> <ul> <li><code>--mem</code> units default to megabytes unless you specify <code>G</code> for gigabytes.</li> <li><code>--time</code> should reflect the maximum expected runtime.</li> <li><code>--cpus-per-task</code> should match the number of threads used by your program.</li> </ul>"},{"location":"running-jobs/determining-resource-requirements.html#verification","title":"Verification","text":"<p>After updating your job script:</p> <ol> <li>Re-submit the job.</li> <li>Confirm successful completion.</li> <li>Review <code>seff</code> or <code>sacct</code> output to ensure your estimates were adequate.</li> <li>Adjust further if needed.</li> </ol> <p>If the job failed due to memory or runtime limits, increment your requests incrementally and re-test.</p>"},{"location":"running-jobs/determining-resource-requirements.html#example-of-updated-job-script","title":"Example of Updated Job Script","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=final_run\n#SBATCH --output=final_run.out\n#SBATCH --error=final_run.err\n#SBATCH --time=00:10:00\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=200M\n\nmodule load python\n\nsrun python production_script.py\n</code></pre> <p>If you have any questions about estimating resource requirements or interpreting SLURM job reports, please contact REPACSS support.</p>"},{"location":"running-jobs/examples.html","title":"Example Job Scripts","text":"<p>This page provides structured job script examples adapted for REPACSS. For definitions and scheduler behavior, refer to the Job Basics page. These examples are designed for CPU and GPU partitions such as <code>zen4</code>, <code>h100</code>, and <code>standard</code>.</p> <p>Tip</p> <p>Interactive jobs in GPU partitions are granted scheduling priority on REPACSS.</p> <p>Warning</p> <p>Each Slurm CPU is a hyperthread. To bind OpenMP threads to physical cores, use <code>--cpu-bind=cores</code>.</p> <p>Note</p> <p>To run jobs in parallel, use <code>&amp;</code>, and use <code>wait</code> to synchronize them.</p>"},{"location":"running-jobs/examples.html#job-types","title":"Job Types","text":"<p>Jobs on REPACSS can be submitted in two main forms:</p> <ul> <li>Interactive Jobs: Real-time sessions for testing and debugging.</li> </ul> <pre><code>interactive -c 8 -p h100\n</code></pre> <ul> <li>Batch Jobs: Scheduled jobs submitted via script.</li> </ul> <pre><code>sbatch job.sh\nsbatch -p zen4 job.sh\nsbatch -p h100 job.sh\n</code></pre>"},{"location":"running-jobs/examples.html#script-templates","title":"Script Templates","text":""},{"location":"running-jobs/examples.html#basic-job-script-with-c","title":"Basic Job Script with C","text":"<ol> <li> <p>Create a file named <code>my_program.c</code> with the following content: <pre><code>#include &lt;stdio.h&gt;\n#include &lt;unistd.h&gt;\n\nint main() {\n    printf(\"SLURM job started.\\n\");\n    printf(\"Sleeping for 60 seconds to simulate work...\\n\");\n    sleep(60);\n    printf(\"Job complete. Goodbye!\\n\");\n    return 0;\n}\n</code></pre></p> </li> <li> <p>Load the GCC module and compile the program: <pre><code>module load gcc/14.2.0\ngcc my_program.c -o my_program\n</code></pre></p> </li> <li> <p>Create a file named <code>submit_job.sh</code> with the following contents: <pre><code>#!/bin/bash\n#SBATCH --job-name=test\n#SBATCH --output=test.out\n#SBATCH --error=test.err\n#SBATCH --time=01:00:00\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=4G\n\n# Load modules\nmodule load gcc/14.2.0\n\n# Run program\n./my_program\n</code></pre> To determine your resource needs, refer to the Determine Resource Needs documentation.</p> </li> <li> <p>Make the script executable and submit it using <code>sbatch</code>: <pre><code>sbatch submit_job.sh\n</code></pre></p> </li> </ol>"},{"location":"running-jobs/examples.html#python-job-script","title":"Python Job Script","text":"<ol> <li> <p>Create a Python file named <code>script.py</code> with the following example content: <pre><code>import time\nimport platform\n\nprint(\"SLURM Python job started.\")\nprint(\"Running on:\", platform.node())\ntime.sleep(10)  # Simulate workload\nprint(\"Job complete. Goodbye!\")\n</code></pre></p> </li> <li> <p>Create a file named <code>submit_python_job.sh</code> with the following content: <pre><code>#!/bin/bash\n#SBATCH --job-name=python_job\n#SBATCH --output=python_job.out\n#SBATCH --error=python_job.err\n#SBATCH --time=01:00:00\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=32G\n\n# Load required modules\nmodule load gcc\n\n# Activate conda environment\nsource ~/miniforge3/etc/profile.d/conda.sh\nconda activate myenv\n\n# Run Python script\npython script.py\n</code></pre> To determine your resource needs, refer to the Determine Resource Needs documentation.</p> </li> <li> <p>Make the script executable and submit it to SLURM: <pre><code>sbatch submit_python_job.sh\n</code></pre></p> </li> </ol>"},{"location":"running-jobs/examples.html#gpu-job-script","title":"GPU Job Script","text":"<p>Warning</p> <p>We are currently working to make the CUDA module available system-wide for all users. In the meantime, please use CUDA via a Conda environment as described below.</p> <ol> <li> <p>Create and activate a new Conda environment <pre><code>conda create --name cuda-env python=3.10 -y\nconda activate cuda-env\n</code></pre></p> </li> <li> <p>Install the CUDA Toolkit with nvcc support <pre><code>conda install -c nvidia cuda-toolkit=12.9\n</code></pre></p> </li> <li> <p>Install a compatible GCC toolchain (GCC 11) <pre><code>conda install -c conda-forge gxx_linux-64=11\n</code></pre></p> </li> <li> <p>Create a sample CUDA program: gpu_program.cu <pre><code>#include &lt;stdio.h&gt;\n#include &lt;cuda_runtime.h&gt;\n\n__global__ void hello_from_gpu() {\n    printf(\"Hello from GPU thread %d!\\n\", threadIdx.x);\n}\n\nint main() {\n    printf(\"Starting GPU job...\\n\");\n    hello_from_gpu&lt;&lt;&lt;1, 64&gt;&gt;&gt;();\n\n    cudaError_t err = cudaGetLastError();\n    if (err != cudaSuccess) {\n        fprintf(stderr, \"Kernel launch failed: %s\\n\", cudaGetErrorString(err));\n        return 1;\n    }\n\n    err = cudaDeviceSynchronize();\n    if (err != cudaSuccess) {\n        fprintf(stderr, \"CUDA error after kernel: %s\\n\", cudaGetErrorString(err));\n        return 1;\n    }\n\n    printf(\"GPU job finished.\\n\");\n    return 0;\n}\n</code></pre></p> </li> <li> <p>Compile the CUDA program for NVIDIA H100 GPUs (sm_90) <pre><code>nvcc -arch=sm_90 \\\n  -ccbin \"$CONDA_PREFIX/bin/x86_64-conda-linux-gnu-g++\" \\\n  -I\"$CONDA_PREFIX/targets/x86_64-linux/include\" \\\n  -L\"$CONDA_PREFIX/targets/x86_64-linux/lib\" \\\n  -o gpu_program gpu_program.cu\n</code></pre></p> </li> <li> <p>Create the SLURM job script: <code>gpu_job.slurm</code> <pre><code>#!/bin/bash\n#SBATCH --job-name=gpu_hello\n#SBATCH --output=gpu_hello.out\n#SBATCH --error=gpu_hello.err\n#SBATCH --partition=h100\n#SBATCH --gres=gpu:nvidia_h100_nvl:1\n#SBATCH --cpus-per-task=2\n#SBATCH --mem=4G\n#SBATCH --time=00:05:00\n\nsource ~/miniforge3/etc/profile.d/conda.sh\nconda activate cuda-env\n\n./gpu_program\n</code></pre> To determine your resource needs, refer to the Determine Resource Needs documentation.</p> </li> <li> <p>Submit the job to SLURM <pre><code>sbatch gpu_job.slurm\n</code></pre></p> </li> </ol>"},{"location":"running-jobs/examples.html#job-management","title":"Job Management","text":""},{"location":"running-jobs/examples.html#submission","title":"Submission","text":"<pre><code>sbatch job.sh                       # Submit job\nsbatch --array=1-10 job.sh          # Submit job array\nsbatch --dependency=afterok:12345 job.sh  # Submit with dependency\n</code></pre>"},{"location":"running-jobs/examples.html#monitoring","title":"Monitoring","text":"<pre><code>squeue -u $USER           # View user's jobs\nsqueue -p zen4            # View jobs on zen4 partition\nsqueue -p h100            # View jobs on h100 partition\n</code></pre>"},{"location":"running-jobs/examples.html#control","title":"Control","text":"<pre><code>scancel 12345             # Cancel a specific job\nscancel -u $USER          # Cancel all user's jobs\nscancel -p zen4           # Cancel all jobs in zen4 partition\n</code></pre>"},{"location":"running-jobs/examples.html#resource-requests","title":"Resource Requests","text":"<ul> <li>CPU Jobs: Specify <code>--nodes</code>, <code>--ntasks</code>, <code>--cpus-per-task</code>, and <code>--mem</code></li> <li>GPU Jobs: Include <code>--gres=gpu:1</code> or more as needed</li> <li> <p>Python Jobs:</p> </li> <li> <p>See Python environment setup</p> </li> <li>Use <code>--cpus-per-task</code> for multi-threading</li> <li>Set <code>--mem</code> appropriately for data requirements</li> </ul> <p>For additional guidance, consult Slurm Documentation and REPACSS-specific resources.</p>"},{"location":"running-jobs/interactive.html","title":"Interactive Sessions","text":"<p>This section provides official guidance on initiating interactive sessions on the REPACSS high-performance computing system. Interactive sessions allow users to request computational resources in real time and execute commands directly on compute nodes. This method is particularly suitable for software testing, debugging, exploratory tasks, and graphical application workflows.</p> <p>Warning</p> <p>Interactive sessions should be used exclusively for development and debugging purposes. Once your job is ready for full execution, please submit it using a SLURM batch job and <code>exit</code> from the resources.</p>"},{"location":"running-jobs/interactive.html#requesting-interactive-resources","title":"Requesting Interactive Resources","text":"<p>To start an interactive session, use the <code>interactive</code> command. This command wraps around the resource request process, performs additional setup, and launches a shell on a compute node once resources are allocated.</p> <pre><code>interactive -c 8 -p h100\n</code></pre> <p>This method is the recommended way to start interactive sessions on REPACSS.</p> <p>Tip</p> <p>Do not call <code>salloc</code> directly unless instructed otherwise. The <code>interactive</code> command performs important environment configuration steps that <code>salloc</code> alone does not handle.</p>"},{"location":"running-jobs/interactive.html#interactive-sessions-on-gpu-nodes","title":"Interactive Sessions on GPU Nodes","text":"<p>When working with graphical processing units (GPUs), use the <code>interactive</code> command with the appropriate GPU partition and core count. For example:</p> <pre><code>interactive -c 8 -p h100\n</code></pre> <p>Within the session, use <code>srun</code> to launch your GPU applications:</p> <pre><code>srun --gres=gpu:nvidia_h100_nvl:1 ./my_gpu_program\n</code></pre> <p>Warning</p> <p>If GPU resources are not explicitly requested, applications relying on GPU acceleration (e.g., CUDA) may fail with an error such as:</p> <pre><code>no CUDA-capable device is detected\n</code></pre>"},{"location":"running-jobs/interactive.html#interactive-sessions-on-cpu-only-nodes","title":"Interactive Sessions on CPU-Only Nodes","text":"<p>For CPU-only workloads, use the <code>interactive</code> command with a CPU-only partition such as <code>zen4</code>:</p> <pre><code>interactive -c 8 -p zen4\n</code></pre> <p>Once the session begins, you can run programs directly or use <code>srun</code> for launching parallel tasks:</p> <pre><code>srun ./my_cpu_program\n</code></pre>"},{"location":"running-jobs/interactive.html#resource-allocation-timeout-and-immediate-scheduling","title":"Resource Allocation Timeout and Immediate Scheduling","text":"<p>By default, interactive job requests will time out if resource allocation is not completed within six (6) minutes. If needed, a custom wait limit can be applied using:</p> <pre><code>interactive -c 8 -p zen4 --immediate=600\n</code></pre> <p>This example waits up to 600 seconds (10 minutes) for compute resources.</p> <p>Note</p> <p>If your connection is lost, the interactive session will terminate and any unsaved progress may be lost.</p>"},{"location":"running-jobs/interactive.html#common-issues-and-resolutions","title":"Common Issues and Resolutions","text":"<ul> <li>GPU Execution Errors: Confirm that GPU resources are requested in the interactive session and in any <code>srun</code> commands.</li> <li>Invalid Account Settings: Contact REPACSS Support.</li> <li>Unavailable Resources: If nodes are unavailable, consider lowering resource requests or choosing a less busy partition.</li> </ul>"},{"location":"running-jobs/interactive.html#additional-documentation","title":"Additional Documentation","text":"<ul> <li>Running Jobs </li> <li>Job Queues and Scheduling </li> <li>Example Job Scripts</li> </ul>"},{"location":"running-jobs/monitoring.html","title":"Monitoring Jobs","text":"<p>Note</p> <p>Avoid running multiple instances of <code>watch squeue</code> or <code>watch sqs</code>. This can overload the scheduler, which is a shared system resource. If you must use watch, use <code>watch -n 60</code> and stop the process when finished.</p>"},{"location":"running-jobs/monitoring.html#using-squeue","title":"Using <code>squeue</code>","text":"<p>The <code>squeue</code> command provides real-time job queue information directly from the Slurm scheduler. It is helpful for checking the current state of jobs, such as <code>PENDING</code>, <code>RUNNING</code>, or <code>COMPLETED</code>.</p> <pre><code>squeue --me          # Shows your jobs\nsqueue -u $USER      # Equivalent to --me\nsqueue --me -t R     # Only running jobs\nsqueue --me -t PD    # Only pending jobs\nsqueue -j 1234,1235  # Filter by job IDs\n</code></pre> <p>To show jobs by account:</p> <pre><code>squeue -A your_project_name\n</code></pre> <p>To view job steps:</p> <pre><code>squeue --steps 1001.0\n</code></pre>"},{"location":"running-jobs/monitoring.html#using-sacct","title":"Using <code>sacct</code>","text":"<p>The <code>sacct</code> command retrieves accounting information about active and completed jobs.</p> <p>Basic usage:</p> <pre><code>sacct\n</code></pre> <p>Customize output by specifying fields:</p> <pre><code>sacct --format=JobID,JobName,State,Start,Elapsed\n</code></pre> <p>Filter jobs by date:</p> <pre><code>sacct -S 2024-06-01 -E 2024-06-13\n</code></pre> <p>Display only failed jobs:</p> <pre><code>sacct -X --format=User,JobName,State -s F --start=2024-06-01 --end=now\n</code></pre> <p>Filter by specific job IDs:</p> <pre><code>sacct -j 123456,123457\n</code></pre>"},{"location":"running-jobs/monitoring.html#using-sstat","title":"Using <code>sstat</code>","text":"<p>Use <code>sstat</code> to report resource usage for jobs that are currently running:</p> <pre><code>sstat -j 123456 -o JobID,MaxRSS\n</code></pre>"},{"location":"running-jobs/monitoring.html#using-jobstats","title":"Using <code>jobstats</code>","text":"<p><code>jobstats</code> is a Python-based reporting tool that summarizes job activity using data from <code>sacct</code>, <code>squeue</code>, and <code>sreport</code>.</p> <pre><code>module load python\njobstats\n</code></pre> <p>Example usage:</p> <pre><code>jobstats --user bsencer --start 2025-06-01 --end 2025-06-13\n</code></pre> <p>To display all options:</p> <pre><code>jobstats --help\n</code></pre>"},{"location":"running-jobs/monitoring.html#email-notifications","title":"Email Notifications","text":"<p>To receive notifications when your job begins, ends, or fails, add the following directives to your Slurm job script:</p> <pre><code>#SBATCH --mail-type=begin,end,fail\n#SBATCH --mail-user=your@email.com\n</code></pre>"},{"location":"running-jobs/monitoring.html#modifying-or-canceling-jobs","title":"Modifying or Canceling Jobs","text":"<p>To cancel a job:</p> <pre><code>scancel 123456\n</code></pre> <p>To cancel multiple jobs:</p> <pre><code>scancel 123456 123457\n</code></pre> <p>To cancel all jobs submitted by your user:</p> <pre><code>scancel -u $USER\n</code></pre> <p>To update a job\u2019s time limit:</p> <pre><code>scontrol update jobid=123456 timelimit=02:00:00\n</code></pre>"},{"location":"running-jobs/monitoring.html#holding-releasing-and-requeuing-jobs","title":"Holding, Releasing, and Requeuing Jobs","text":"<p>Place a job on hold (prevent scheduling):</p> <pre><code>scontrol hold 123456\n</code></pre> <p>Release a held job:</p> <pre><code>scontrol release 123456\n</code></pre> <p>Requeue a job (e.g., after failure or timeout):</p> <pre><code>scontrol requeue 123456\n</code></pre>"},{"location":"running-jobs/scheduling.html","title":"Job Queues on REPACSS","text":"<p>This page provides a comprehensive explanation of job queuing mechanisms on REPACSS and guidance for users on how to effectively manage their job submissions.</p>"},{"location":"running-jobs/scheduling.html#queue-overview","title":"Queue Overview","text":"<p>When a user submits a job to the REPACSS system, it is placed into a queue controlled by the Slurm workload manager. This queue is governed by a priority-based scheduling system, where jobs are executed based on criteria such as resource availability, job size, requested walltime, and policy-defined priorities.</p> <p>Although all jobs are executed on the same underlying compute nodes, the order in which they begin is determined by these scheduling policies. Some jobs, especially those with smaller resource demands or shorter walltimes, may experience shorter wait times.</p>"},{"location":"running-jobs/scheduling.html#queue-wait-times","title":"Queue Wait Times","text":"<p>It is typical for submitted jobs to remain in the queue for a duration longer than their execution time. Factors influencing wait times include:</p> <ul> <li>Current system workload</li> <li>Availability of requested resources (e.g., nodes, CPUs, GPUs)</li> <li>Size and duration of the job</li> <li>Assigned job priority</li> </ul> <p>Smaller or flexible jobs may benefit from earlier scheduling through the backfill mechanism.</p>"},{"location":"running-jobs/scheduling.html#viewing-the-queue","title":"Viewing the Queue","text":"<p>Users may inspect the status of their queued jobs with the following command:</p> <pre><code>squeue -u $USER\n</code></pre> <p>To estimate when a job is expected to start, use:</p> <pre><code>squeue --start -j &lt;job_id&gt;\n</code></pre> <p>Pending jobs will include a status reason under the <code>NODELIST(REASON)</code> column, indicating why execution has not yet begun (e.g., \"Resources\", \"Priority\", or \"Dependency\").</p>"},{"location":"running-jobs/scheduling.html#scheduling-mechanics","title":"Scheduling Mechanics","text":"<p>REPACSS employs Slurm to manage job execution and optimize resource utilization.</p>"},{"location":"running-jobs/scheduling.html#priority-and-job-aging","title":"Priority and Job Aging","text":"<p>Each job is assigned a priority value that increases over time through a mechanism known as aging. This process ensures that jobs do not remain indefinitely in the queue. However, to maintain fairness, only a limited number of jobs per user can accumulate priority concurrently.</p> <p>This approach prevents individual users from monopolizing the scheduling queue and promotes equitable access for all users.</p>"},{"location":"running-jobs/scheduling.html#scheduling-algorithms","title":"Scheduling Algorithms","text":"<p>Slurm uses two complementary scheduling strategies:</p> <ul> <li>Immediate Scheduling: Rapidly assembles a tentative schedule using the highest-priority jobs.</li> <li>Backfill Scheduling: Identifies and executes smaller jobs that can be run in time gaps without delaying higher-priority jobs.</li> </ul> <p>This hybrid model enables efficient system utilization while allowing for the timely execution of short-duration tasks. Because the schedule is recalculated frequently, job start times shown by the scheduler may fluctuate.</p>"},{"location":"running-jobs/scheduling.html#recommendations-for-efficient-scheduling","title":"Recommendations for Efficient Scheduling","text":"<p>To minimize queue wait times and maximize job throughput, users are encouraged to:</p> <ul> <li>Provide accurate resource and time estimates</li> <li>Avoid requesting excessive compute resources</li> <li>Utilize interactive or shared job modes for testing</li> <li>Decompose large workflows into smaller, manageable tasks</li> <li>Employ job arrays for repetitive or parameterized workloads</li> </ul>"},{"location":"running-jobs/scheduling.html#interpreting-job-states","title":"Interpreting Job States","text":"<p>Below is a summary of common Slurm job states:</p> State Description <code>PENDING (PD)</code> Waiting for resources to become available <code>RUNNING (R)</code> Currently executing on allocated resources <code>COMPLETED (CD)</code> Successfully finished execution <code>FAILED (F)</code> Terminated due to an error <code>CANCELLED (CA)</code> Cancelled by the user or system administrator <code>TIMEOUT (TO)</code> Exceeded requested walltime <p>To view historical job information:</p> <pre><code>sacct -u $USER --starttime today\n</code></pre>"},{"location":"running-jobs/scheduling.html#additional-support","title":"Additional Support","text":"<p>If a job remains in the queue unexpectedly or assistance is needed for job optimization, users may contact the system administrators or file a support request.</p> <p>Help documentation for Slurm commands is also available:</p> <pre><code>squeue --help\nsacct --help\n</code></pre> <p>Related documentation includes:</p> <ul> <li>Job Basics</li> <li>Example Scripts</li> <li>Monitoring Tools</li> </ul>"},{"location":"software/conda.html","title":"Getting Started with Conda at REPACSS","text":""},{"location":"software/conda.html#introduction","title":"Introduction","text":"<p>It is often useful to set up a fully customized set of software packages including Python and many other utility tools using the MiniForge package manager. For performance, consistency, and licensing flexibility, we recommend using MiniForge instead of tools distributed by Anaconda, Inc. such as Anaconda or Miniconda.</p> <p>While Anaconda provides a broad suite of tools, it uses proprietary channels that can lead to outdated or restricted packages. MiniForge is a minimal, community-maintained Conda distribution that defaults to the conda-forge channel, providing access to the latest packages without commercial licensing concerns. MiniForge also includes the Mamba backend for faster environment solving and installation.</p> <p>This guide explains the benefits of using MiniForge on REPACSS and provides step-by-step instructions for setup and environment management.</p>"},{"location":"software/conda.html#overview-of-conda-ecosystem","title":"Overview of Conda Ecosystem","text":"<ul> <li>Conda: Cross-platform command-line tool for managing packages and environments.</li> <li>Anaconda: A commercial distribution that includes Conda and many preinstalled data science packages.</li> <li>Miniconda: A minimal Anaconda installer with basic tools and default channels.</li> <li>MiniForge: A minimal installer with only conda-forge preconfigured and no proprietary channels.</li> <li>Mamba: A drop-in replacement for Conda, built in C++ for speed and efficiency.</li> </ul>"},{"location":"software/conda.html#why-use-miniforge","title":"Why Use MiniForge","text":"<ul> <li>Community-Centric: Uses conda-forge as the default source of packages.</li> <li>No Default Channel Conflicts: Avoids Anaconda's proprietary defaults.</li> <li>Minimal Footprint: Lets you install only what you need.</li> <li>Fast and Reliable: Includes Mamba for faster package resolution.</li> <li>Licensing Clarity: No commercial restrictions.</li> </ul>"},{"location":"software/conda.html#removing-previous-conda-installations","title":"Removing Previous Conda Installations","text":"<p>If you previously installed Anaconda or Miniconda, we recommend removing them to avoid conflicts.</p>"},{"location":"software/conda.html#check-for-existing-conda","title":"Check for existing Conda","text":"<pre><code>conda --version\n</code></pre>"},{"location":"software/conda.html#locate-old-installations","title":"Locate old installations","text":"<pre><code>ls -al | grep conda\n</code></pre>"},{"location":"software/conda.html#remove-directories","title":"Remove directories","text":"<pre><code>find . -maxdepth 1 -name '*conda*' -exec rm -ir {} +\nfind . -maxdepth 1 -name 'Miniforge*' -exec rm -ir {} +\nfind . -maxdepth 1 -name 'Mambaforge*' -exec rm -ir {} +\n</code></pre>"},{"location":"software/conda.html#clean-bashrc","title":"Clean <code>.bashrc</code>","text":"<p>Remove any <code>conda</code> initialization lines and sign out and back in.</p>"},{"location":"software/conda.html#installing-miniforge","title":"Installing MiniForge","text":""},{"location":"software/conda.html#download-and-run-installer","title":"Download and run installer","text":"<pre><code>wget https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh\nbash Miniforge3-$(uname)-$(uname -m).sh\n</code></pre> <p>Accept the license agreement and install to the default directory (e.g., <code>$HOME/miniforge3</code>).</p>"},{"location":"software/conda.html#initialize-and-prevent-auto-activation","title":"Initialize and prevent auto-activation","text":"<pre><code>source ~/.bashrc\nconda config --set auto_activate_base false\n</code></pre>"},{"location":"software/conda.html#confirm-installation","title":"Confirm installation","text":"<pre><code>which conda\nconda --version\n</code></pre>"},{"location":"software/conda.html#adding-channels-optional","title":"Adding Channels (Optional)","text":"<p>By default, MiniForge uses <code>conda-forge</code>. Additional channels can be added if needed.</p>"},{"location":"software/conda.html#set-strict-channel-priority","title":"Set strict channel priority","text":"<pre><code>conda config --set channel_priority strict\n</code></pre>"},{"location":"software/conda.html#optional-add-bioconda-for-bioinformatics","title":"Optional: Add Bioconda (for bioinformatics)","text":"<pre><code>conda config --add channels bioconda\n</code></pre>"},{"location":"software/conda.html#creating-virtual-environments","title":"Creating Virtual Environments","text":""},{"location":"software/conda.html#create-a-new-environment","title":"Create a new environment","text":"<pre><code>conda create -n &lt;env_name&gt; python=3.11 numpy scipy matplotlib\n</code></pre>"},{"location":"software/conda.html#activate-the-environment","title":"Activate the environment","text":"<pre><code>conda activate &lt;env_name&gt;\n</code></pre>"},{"location":"software/conda.html#list-environments","title":"List environments","text":"<pre><code>conda env list\n</code></pre>"},{"location":"software/conda.html#managing-and-removing-environments","title":"Managing and Removing Environments","text":""},{"location":"software/conda.html#deactivate-the-current-environment","title":"Deactivate the current environment","text":"<pre><code>conda deactivate\n</code></pre>"},{"location":"software/conda.html#remove-an-environment","title":"Remove an environment","text":"<pre><code>conda env remove --name &lt;env_name&gt;\n</code></pre>"},{"location":"software/conda.html#optimizing-conda-initialization","title":"Optimizing Conda Initialization","text":"<p>Avoid installing all packages in the <code>base</code> environment. Create small, purpose-specific environments for each project.</p>"},{"location":"software/conda.html#prevent-base-from-activating-on-login","title":"Prevent <code>base</code> from activating on login","text":"<pre><code>conda config --set auto_activate_base false\n</code></pre>"},{"location":"software/conda.html#check-the-configuration","title":"Check the configuration","text":"<pre><code>conda config --show | grep auto_activate_base\n</code></pre>"},{"location":"software/conda.html#notes-on-anaconda-licensing","title":"Notes on Anaconda Licensing","text":"<p>Anaconda, Inc. enforces licensing terms on their default channels and tools. Usage in commercial or research contexts may require a paid license.</p> <p>To avoid licensing complications and ensure access to current packages, we recommend avoiding the Anaconda defaults and using MiniForge + conda-forge.</p> <p>Warning</p> <p>REPACSS does not cover licensing costs related to Anaconda. Users are responsible for any commercial usage compliance.</p> <p>For more details, see: Anaconda Terms of Service</p>"},{"location":"software/installing-packages.html","title":"Installing User-Level Software Packages","text":"<p>This document provides guidance for installing additional software packages, particularly Python libraries, within the REPACSS computing environment. Users are permitted to install software in their own workspace without requiring administrative privileges, provided installations are performed in designated directories such as <code>$HOME</code> or <code>$WORK</code>.</p>"},{"location":"software/installing-packages.html#installing-miniforge-python-311","title":"Installing Miniforge (Python 3.11)","text":"<p>Miniforge is a minimal Conda-based Python distribution available for local installation. To install Miniforge in your home directory:</p> <ol> <li> <p>Download the installer:    <pre><code>wget https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh\n</code></pre></p> </li> <li> <p>Run the installer:    <pre><code>bash Miniforge3-$(uname)-$(uname -m).sh\n</code></pre>    When prompted, accept the license agreement and install to a directory under <code>$HOME</code>, for example <code>$HOME/miniforge3</code>.   </p> </li> <li> <p>Initialize Conda:    <pre><code>source ~/miniforge3/etc/profile.d/conda.sh\nconda init\n</code></pre></p> </li> <li> <p>Restart your shell or manually activate Conda:    <pre><code>source ~/.bashrc    # or ~/.zshrc\nconda activate base\n</code></pre></p> </li> </ol>"},{"location":"software/installing-packages.html#python-package-management","title":"Python Package Management","text":"<p>Miniforge provides Python 3.11 and supports isolated environments using <code>conda</code>.</p>"},{"location":"software/installing-packages.html#creating-and-managing-environments","title":"Creating and Managing Environments","text":"<p>To create a new conda environment with selected packages:</p> <pre><code>conda create -n myenv numpy scipy matplotlib\nconda activate myenv\n</code></pre> <p>To install more packages into an existing environment:</p> <pre><code>conda activate myenv\nconda install pandas scikit-learn\n</code></pre> <p>To deactivate or remove an environment:</p> <pre><code>conda deactivate\nconda remove --name myenv --all\n</code></pre> <p>Installing CUDA</p> <p>You can install the CUDA Toolkit from either conda-forge or NVIDIA channels. Replace <code>x</code> with the minor version you need (e.g., <code>12.2</code>).</p> <pre><code># Install from conda-forge\nconda install cudatoolkit=12.x -c conda-forge\n\n# OR install from NVIDIA channel\nconda install cudatoolkit=12.x -c nvidia\n</code></pre>"},{"location":"software/installing-packages.html#best-practices-and-guidelines","title":"Best Practices and Guidelines","text":"<ul> <li>All software should be installed to user-controlled directories such as <code>$HOME</code> or <code>$WORK</code>. Installation to system directories is not permitted.</li> <li>Use <code>conda</code> environments or <code>venv</code> for managing dependencies and improving reproducibility.</li> <li>For large-scale or portable workflows, consider encapsulating environments using containers (e.g., Apptainer/Singularity).</li> <li>Maintain separate environments for distinct projects to prevent dependency conflicts.</li> </ul>"},{"location":"software/installing-packages.html#additional-resources","title":"Additional Resources","text":"<p>For further configuration or software access guidance, refer to the following documentation:</p> <ul> <li>Module System</li> <li>Available Software</li> </ul> <p>For additional support or to request software installation at the system level, please contact:</p> <pre><code>repacss.support@ttu.edu\n</code></pre>"},{"location":"software/module-system.html","title":"Using Software Modules on REPACSS","text":"<p>REPACSS offers a wide selection of research and instructional software using the Lmod environment module system. This guide shows you how to find, load, and manage software modules, and provides an up-to-date summary of available software environments.</p>"},{"location":"software/module-system.html#available-software-modules","title":"Available Software Modules","text":"<p>Software modules are grouped by repository path and hardware partition. You can load any module on any partition, but only GPU-enabled packages can utilize GPU resources when running on the GPU (h100) partition. Some modules (such as MPI libraries) are available in both partitions, but with different capabilities.</p>"},{"location":"software/module-system.html#zen4-partition","title":"zen4 Partition","text":""},{"location":"software/module-system.html#optappsnfsmoduleszen4rocky94core","title":"<code>/opt/apps/nfs/modules/zen4/rocky9.4/Core</code>","text":"Module Version Description boost 1.86.0 C++ libraries for tasks and structures bwa 0.7.17 Burrows-Wheeler Aligner for sequence alignment eigen 3.4.0 C++ template library for linear algebra ffmpeg 6.1.1 Audio and video processing tools gdal 3.10.0 Geospatial Data Abstraction Library geos 3.13.0 Geometry Engine - Open Source gcc 14.2.0 GNU Compiler Collection (C, C++, Fortran) gsl 2.8 GNU Scientific Library gnuplot 6.0.0 Portable command-line driven graphing utility htop 3.3.0 Interactive process viewer imagemagick 7.1.1-39 Image manipulation tools intel-oneapi-mkl 2024.2.2 Intel Math Kernel Library intel-oneapi-mpi 2021.4.0 Intel MPI Library metis 5.1.0 Serial graph partitioning and fill-reducing matrix ordering mpich 4.1.2 MPI implementation (default) ninja 1.12.1 Small build system with a focus on speed openblas 0.3.28 Optimized BLAS library openmpi 4.1.6 High-performance MPI (default) proj 9.4.1 Cartographic projections and coordinate transformations python 3.10.10 Python programming language python 3.12.5 Python programming language (default) r 4.4.1 Statistical computing and graphics samtools 1.19.2 Tools for manipulating next-generation sequencing data sparsehash 2.0.4 Memory-efficient hash map implementation swig 4.1.0-fortran Simplified Wrapper and Interface Generator for Fortran tmux 3.4 Terminal multiplexer"},{"location":"software/module-system.html#optappsnfsmoduleszen4rocky94mpich412-a5xh3gecore","title":"<code>/opt/apps/nfs/modules/zen4/rocky9.4/mpich/4.1.2-a5xh3ge/Core</code>","text":"<p>Note</p> <p>These modules are only available after loading the <code>mpich</code> module (default in zen4). Run:</p> <p><pre><code>module load mpich\n</code></pre> before using any of these packages. To make sure you are loading the default zen4 mpich, not the h100 mpich. After loading, check the module path with:</p> <p><pre><code>module show mpich\n</code></pre> The path should include <code>/opt/apps/nfs/modules/zen4/rocky9.4/mpich/4.1.2-a5xh3ge/Core</code>.</p> Module Version Description fftw 3.3.10 Fast Fourier Transform library used in signal and image processing hdf5 1.14.5 Hierarchical Data Format library for large/complex data lammps 20240829.1 Molecular dynamics simulator netcdf-c 4.9.2 Data format for array-oriented scientific data p3dfft3 3.0.0 Parallel 3D FFT library quantum-espresso 7.4 Electronic-structure and materials modeling suite scotch 7.0.4 Graph partitioning, static mapping, and clustering library wrf 4.6.1 Weather Research and Forecasting Model wps 4.5 WRF Preprocessing System"},{"location":"software/module-system.html#h100-gpu-partition","title":"h100 (GPU Partition)","text":""},{"location":"software/module-system.html#optappsnfsmodulesh100rocky94core","title":"<code>/opt/apps/nfs/modules/h100/rocky9.4/Core</code>","text":"<p>These modules are GPU-enabled. To access them, prepend the h100 path to your MODULEPATH (see MPI section below):</p> Module Version Description cuda 12.6.2 NVIDIA CUDA Toolkit cudnn 9.2.0.82-12 NVIDIA CUDA Deep Neural Network library mpich 4.1.2 MPI implementation (GPU-enabled) nccl 2.22.3-1 NVIDIA Collective Communications Library openmpi 4.1.6 High-performance implementation of MPI (GPU-enabled)"},{"location":"software/module-system.html#mpi-implementations-and-usage-guidance","title":"MPI Implementations and Usage Guidance","text":""},{"location":"software/module-system.html#overview","title":"Overview","text":"<p>Two MPI implementations are available: OpenMPI and MPICH. Each is provided for both CPU (zen4) and GPU (h100) partitions. The h100 versions are CUDA-enabled for GPU workloads.</p> <ul> <li>zen4: CPU-only MPI modules (default: mpich, openmpi)</li> <li>h100: GPU-enabled MPI modules (mpich, openmpi)</li> </ul>"},{"location":"software/module-system.html#using-mpi-dependent-packages","title":"Using MPI-Dependent Packages","text":"<p>Some software (e.g., <code>fftw</code>, <code>hdf5</code>, <code>lammps</code>, etc.) in the zen4 partition (see the table above) requires the <code>mpich</code> module to be loaded first. To do this, run:</p> <pre><code>module load mpich\n</code></pre> <p>To make sure you are loading the default zen4 mpich, not the h100 mpich. You can verify this with:</p> <pre><code>module show mpich\n</code></pre> <p>The output should reference <code>/opt/apps/nfs/modules/zen4/rocky9.4/mpich/4.1.2-a5xh3ge/Core</code>.</p> <p>This ensures the correct MPI environment for these libraries.</p>"},{"location":"software/module-system.html#accessing-gpu-enabled-mpi-modules","title":"Accessing GPU-Enabled MPI Modules","text":"<p>To use CUDA-enabled <code>mpich</code> or <code>openmpi</code> in the GPU (h100) partition: 1. Prepend the h100 module path to your MODULEPATH:    <pre><code>module use /opt/apps/nfs/modules/h100/rocky9.4/Core\n</code></pre> 2. Load the desired MPI module (e.g., <code>mpich</code> or <code>openmpi</code>).</p>"},{"location":"software/module-system.html#switching-between-mpi-implementations","title":"Switching Between MPI Implementations","text":"<p>To switch from one MPI implementation to another, use: <pre><code>module swap mpich openmpi\n</code></pre> This unloads the current MPI module and loads the new one, updating your environment automatically.</p>"},{"location":"software/module-system.html#how-to-use-modules","title":"How to Use Modules","text":""},{"location":"software/module-system.html#list-available-modules","title":"List Available Modules","text":"<p>Show all modules you can load:</p> <pre><code>module avail\n</code></pre> <p>Show only default modules:</p> <pre><code>module --default avail\n# or\nml -d av\n</code></pre> <p>See module categories and counts:</p> <pre><code>module overview\n# or\nml ov\n</code></pre>"},{"location":"software/module-system.html#search-for-modules","title":"Search for Modules","text":"<p>Find all available software and extensions:</p> <pre><code>module spider\n</code></pre> <p>Search for a specific module or keyword:</p> <pre><code>module spider python\n</code></pre> <p>Get details for a specific version:</p> <pre><code>module spider python/3.11.0\n</code></pre>"},{"location":"software/module-system.html#loading-and-unloading-modules","title":"Loading and Unloading Modules","text":"<p>Load a module to use its software:</p> <pre><code>module load gcc/14.2.0\nmodule load openmpi/4.1.6\n</code></pre> <p>You can load multiple modules at once:</p> <pre><code>module load gcc/14.2.0 cuda/12.0\n</code></pre> <p>Specifying exact versions is recommended for reproducibility.</p> <p>To remove a specific software module from the current environment:</p> <pre><code>module unload gcc/14.2.0\n</code></pre> <p>To remove all loaded modules and restore a clean environment:</p> <pre><code>module purge\n</code></pre>"},{"location":"software/module-system.html#viewing-module-details-and-loaded-modules","title":"Viewing Module Details and Loaded Modules","text":"<p>See what a module changes in your environment:</p> <pre><code>module show hdf5\n</code></pre> <p>List all modules currently loaded:</p> <pre><code>module list\n</code></pre> <p>This helps with troubleshooting and documenting your environment.</p>"},{"location":"software/module-system.html#best-practices-for-job-scripts","title":"Best Practices for Job Scripts","text":"<ul> <li>Start job scripts with <code>module purge</code> for a clean environment.</li> <li>Load only the modules you need.</li> <li>Record loaded modules for reproducibility.</li> </ul>"},{"location":"software/module-system.html#more-information-and-support","title":"More Information and Support","text":"<p>For a full list of available software, see the tables above.</p> <p>For Python-specific help, see the Installing Packages guide.</p> <p>Need help? Contact REPACSS support:</p> <pre><code>repacss.support@ttu.edu\n</code></pre>"},{"location":"understanding/repacss-system/architecture.html","title":"REPACSS Architecture","text":"<p>This document provides a comprehensive overview of the REPACSS (REmotely-managed Power Aware Computing Systems and Services) high-performance computing (HPC) infrastructure at Texas Tech University. The system is designed to support computationally intensive and data-driven research and is powered by renewable energy sources. REPACSS supports a wide range of scientific workloads, including simulations, artificial intelligence (AI), and large-scale data analytics.</p>"},{"location":"understanding/repacss-system/architecture.html#access-requirements","title":"Access Requirements","text":"<p>To utilize REPACSS resources, users must meet the following prerequisites:</p> <ul> <li>A valid TTU eRaider account (for current users)</li> <li>Access to the GlobalProtect VPN</li> <li>Basic familiarity with HPC concepts</li> <li>Competence with Linux command-line environments</li> </ul>"},{"location":"understanding/repacss-system/architecture.html#compute-resources","title":"Compute Resources","text":"<p>REPACSS consists of a heterogeneous cluster of compute resources optimized for both CPU-intensive and GPU-accelerated workloads.</p>"},{"location":"understanding/repacss-system/architecture.html#cpu-compute-nodes","title":"CPU Compute Nodes","text":"<ul> <li>Total Nodes: 110</li> <li>Processor: Dual AMD EPYC 9754</li> <li>Cores per Node: 256</li> <li>Memory per Node: 1.5 TB DDR5</li> <li>Storage per Node: 1.92 TB NVMe SSD</li> </ul>"},{"location":"understanding/repacss-system/architecture.html#gpu-compute-nodes","title":"GPU Compute Nodes","text":"<ul> <li>Total Nodes: 8</li> <li>Processor: Dual Intel Xeon Gold 6448Y</li> <li>Cores per Node: 64</li> <li>Memory per Node: 512 GB</li> <li>GPUs per Node: 4 \u00d7 NVIDIA H100 NVL (94 GB HBM per GPU)</li> <li>Storage per Node: 1.92 TB SSD</li> </ul>"},{"location":"understanding/repacss-system/architecture.html#login-nodes","title":"Login Nodes","text":"<ul> <li>Total Nodes: 3</li> <li>Processor: Dual AMD EPYC 9254</li> <li>Cores per Node: 48</li> <li>Memory per Node: 256 GB</li> <li>Storage per Node: 1.92 TB NVMe SSD</li> </ul>"},{"location":"understanding/repacss-system/architecture.html#storage-infrastructure","title":"Storage Infrastructure","text":"<p>REPACSS includes nine storage nodes that collectively offer approximately 2.94 PB of multi-tiered storage. These systems utilize both high-speed NVMe drives and large-capacity HDDs to accommodate various storage requirements.</p>"},{"location":"understanding/repacss-system/architecture.html#storage-node-summary","title":"Storage Node Summary","text":"<ul> <li>Processors: Intel Xeon Gold (varied models)</li> <li>Cores per Node: 8 to 32</li> <li>Memory per Node: 512 GB to 1 TB</li> <li>Storage per Node: 25.6 TB to 583.68 TB</li> </ul>"},{"location":"understanding/repacss-system/architecture.html#storage-tiers","title":"Storage Tiers","text":"<ul> <li>Home: Persistent personal storage for user scripts and configuration files.</li> <li>Scratch: High-performance temporary storage space subject to periodic purging.</li> <li>Work: Long-term storage for research outputs and work purposes.</li> </ul>"},{"location":"understanding/repacss-system/architecture.html#network-architecture","title":"Network Architecture","text":"<p>The network backbone of REPACSS supports high-throughput and low-latency communication among compute and storage nodes. * Switching Infrastructure: Dell PowerSwitch S5248-ON and S5232-ON * Storage Interconnect: InfiniBand network * Features:     * Reliable and fast data movement     * Efficient support for multi-node parallel processing (MPI)     * Secure remote access capabilities</p>"},{"location":"understanding/repacss-system/architecture.html#system-specifications-summary","title":"System Specifications Summary","text":"Node Type Total Nodes CPU Model CPUs/Node Cores/Node Memory/Node Storage/Node GPUs/Node GPU Model CPU Nodes 110 AMD EPYC 9754 2 256 1.5 TB DDR5 1.92 TB NVMe - - GPU Nodes 8 Intel Xeon Gold 6448Y 2 64 512 GB 1.92 TB SSD 4 NVIDIA H100 NVL (94 GB HBM) Login Nodes 3 AMD EPYC 9254 2 48 256 GB 1.92 TB NVMe - - Storage Nodes 9 Intel Xeon Gold (varied) 2 8\u201332 512 GB\u20131 TB 25.6\u2013583.68 TB - -"},{"location":"understanding/repacss-system/architecture.html#additional-resources","title":"Additional Resources","text":"<ul> <li>Running Jobs</li> <li>Available Software</li> <li>GPU Job Submission</li> <li>Monitoring and Troubleshooting</li> </ul>"},{"location":"understanding/repacss-system/known-issues.html","title":"Known Issues","text":"<p>Feature Under Development</p> <p>This section is still under development. The REPACSS known issues page will soon include the known issues by the REPACSS Team.</p>"},{"location":"understanding/repacss-system/timeline.html","title":"System Timeline","text":"<p>Feature Under Development</p> <p>This section is still under development. The REPACSS timeline will soon include major upgrades, maintenance schedules, and key historical milestones.</p>"},{"location":"understanding/repacss-system/vendor-bugs.html","title":"Vendor Bugs","text":"<p>Feature Under Development</p> <p>This section is still under development. The REPACSS vendor bugs page will soon bugs caused by our vendors.</p>"},{"location":"understanding/repacss-system/file-system/ACL.html","title":"Access Control Lists (ACLs) - Sharing File Access","text":""},{"location":"understanding/repacss-system/file-system/ACL.html#overview","title":"Overview","text":"<p>This document provides an overview of Access Control Lists (ACLs), which allow fine-grained control over file and directory permissions beyond the standard owner, group, and other model. ACLs are commonly used on REPACSS systems to define access for specific users or groups.</p>"},{"location":"understanding/repacss-system/file-system/ACL.html#what-are-acls","title":"What Are ACLs?","text":"<p>ACLs extend the basic permissions by enabling you to:</p> <ul> <li>Grant different permissions to multiple users or groups.</li> <li>Control access without changing a file's primary ownership or group.</li> <li>Define default permissions for newly created files within a directory.</li> </ul> <p>Each file or directory can have:</p> <ul> <li>A set of access ACLs (current permissions)</li> <li>Optional default ACLs (inherited by new items created inside a directory)</li> </ul>"},{"location":"understanding/repacss-system/file-system/ACL.html#viewing-acls","title":"Viewing ACLs","text":"<p>To see the ACL entries of a file or directory, use the <code>getfacl</code> command:</p> <pre><code>getfacl filename\n</code></pre> <p>Example output:</p> <pre><code># file: example.txt\n# owner: bsencer\n# group: discl\nuser::rw-\nuser:userid:rw-\ngroup::r--\nmask::rw-\nother::---\n</code></pre> <p>Explanation of fields:</p> Field Description user:: Permissions for the file owner user:userid Permissions granted to user <code>userid</code> group:: Permissions for the owning group mask:: Maximum effective permissions for named users and groups other:: Permissions for all other users <p>Note</p> <p>The <code>mask</code> acts as an upper limit. For example, if a user has <code>rwx</code> permissions but the mask is <code>rw-</code>, the effective permissions are <code>rw-</code>.</p>"},{"location":"understanding/repacss-system/file-system/ACL.html#modifying-acls","title":"Modifying ACLs","text":"<p>ACLs are modified using the <code>setfacl</code> command.</p>"},{"location":"understanding/repacss-system/file-system/ACL.html#adding-or-modifying-permissions","title":"Adding or Modifying Permissions","text":"<p>To grant read and write access to a specific user:</p> <pre><code>setfacl -m u:userid:rw filename\n</code></pre> <p>Options:</p> <ul> <li><code>u:USERNAME:PERMISSIONS</code> \u2014 set permissions for a user</li> <li><code>g:GROUPNAME:PERMISSIONS</code> \u2014 set permissions for a group</li> </ul> <p>Examples:</p> <ul> <li>Grant read-only access to group <code>research</code>:</li> </ul> <pre><code>setfacl -m g:research:r filename\n</code></pre> <ul> <li>Update the mask (limit effective permissions):</li> </ul> <pre><code>setfacl -m m:rw filename\n</code></pre>"},{"location":"understanding/repacss-system/file-system/ACL.html#removing-acl-entries","title":"Removing ACL Entries","text":"<p>To remove permissions granted to a user:</p> <pre><code>setfacl -x u:userid filename\n</code></pre> <p>To remove group permissions:</p> <pre><code>setfacl -x g:research filename\n</code></pre>"},{"location":"understanding/repacss-system/file-system/ACL.html#setting-default-acls","title":"Setting Default ACLs","text":"<p>Default ACLs apply automatically to new files and directories created within a directory.</p> <p>Example:</p> <p>Grant default read and write access to user <code>userid</code> for all new files in <code>project_data</code>:</p> <pre><code>setfacl -d -m u:userid:rw project_data/\n</code></pre> <p>Tip</p> <p>Use <code>-R</code> to apply ACLs recursively to all existing files and directories:</p> <pre><code>setfacl -R -m u:userid:rw project_data/\n</code></pre>"},{"location":"understanding/repacss-system/file-system/ACL.html#removing-all-acls","title":"Removing All ACLs","text":"<p>To remove all extended ACL entries and revert to standard permissions:</p> <pre><code>setfacl -b filename\n</code></pre>"},{"location":"understanding/repacss-system/file-system/ACL.html#verifying-effective-permissions","title":"Verifying Effective Permissions","text":"<p>After modifying ACLs, use <code>getfacl</code> to confirm the changes:</p> <pre><code>getfacl filename\n</code></pre> <p>Review the <code>mask</code> entry to ensure the permissions are effective as intended.</p>"},{"location":"understanding/repacss-system/file-system/ACL.html#best-practices","title":"Best Practices","text":"<ul> <li>Use ACLs to manage collaborative access to project directories without changing ownership.</li> <li>Always verify permissions with <code>getfacl</code>.</li> <li>Remove unnecessary ACL entries to maintain security and clarity.</li> <li>Consider setting default ACLs when multiple users create files in shared directories.</li> </ul>"},{"location":"understanding/repacss-system/file-system/ACL.html#summary","title":"Summary","text":"<ul> <li><code>getfacl</code> displays current ACLs.</li> <li><code>setfacl -m</code> adds or modifies ACL entries.</li> <li><code>setfacl -x</code> removes specific ACL entries.</li> <li><code>setfacl -d</code> defines default ACLs for directories.</li> <li><code>setfacl -b</code> clears all ACLs.</li> </ul> <p>For assistance managing ACLs, please contact REPACSS support.</p>"},{"location":"understanding/repacss-system/file-system/file-transfer.html","title":"File Transfer with Globus","text":"<p>Avoid Direct Transfers via Login Nodes</p> <p>Please refrain from using <code>scp</code>, <code>sftp</code>, <code>rsync</code>, or direct connections to the login nodes for data transfer. These methods can degrade system performance and are less efficient compared to Globus Connect.</p>"},{"location":"understanding/repacss-system/file-system/file-transfer.html#overview","title":"Overview","text":"<p>REPACSS supports high-performance data movement through Globus Connect, a robust tool designed to facilitate large-scale file transfers. It provides:</p> <ul> <li>High-speed, reliable transfers</li> <li>Automatic error detection and retry</li> <li>Multiple parallel streams for faster throughput</li> <li>No impact on login node performance</li> <li>User-friendly web-based interface</li> </ul>"},{"location":"understanding/repacss-system/file-system/file-transfer.html#setting-up-globus-connect","title":"Setting Up Globus Connect","text":"<p>Follow these steps to enable file transfers using Globus Connect Personal:</p> <ol> <li> <p>Install Globus Connect Personal on your local machine:</p> </li> <li> <p>Windows Installation Guide</p> </li> <li>macOS Installation Guide</li> <li> <p>Linux Installation Guide</p> </li> <li> <p>Create a Personal Collection:</p> </li> <li> <p>After installation, set up a Globus collection tied to your system.</p> </li> <li> <p>Access REPACSS Data:</p> </li> <li> <p>Set the endpoint to: <code>REPACSS</code></p> </li> <li> <p>Navigate to the appropriate storage paths:</p> <ul> <li>Home: <code>/mnt/GROUPID/home/USERID</code></li> <li>Scratch: <code>/mnt/GROUPID/scratch/USERID</code></li> <li>Work: <code>/mnt/GROUPID/work/USERID</code></li> </ul> </li> </ol>"},{"location":"understanding/repacss-system/file-system/file-transfer.html#transferring-data-between-sites","title":"Transferring Data Between Sites","text":"<p>Many academic and research institutions have established Globus endpoints. To transfer data between REPACSS and other sites or to your own machine:</p> <ol> <li>Use the Globus Web Interface to configure transfers.</li> <li>Specify the collection to be used:</li> <li>REPACSS Endpoint: <code>REPACSS</code></li> <li>Locate and select the file(s) you intend to transfer, then click the <code>Transfer or Sync to...</code> button.</li> <li>Select your destination collection.</li> <li>Specify the target file path and click Start to initiate the transfer.</li> </ol>"},{"location":"understanding/repacss-system/file-system/file-transfer.html#best-practices","title":"Best Practices","text":"<ul> <li>Always verify file integrity post-transfer.</li> <li>Monitor job completion using the Globus web interface.</li> </ul>"},{"location":"understanding/repacss-system/file-system/unix-permissions.html","title":"Unix File Permissions","text":""},{"location":"understanding/repacss-system/file-system/unix-permissions.html#overview","title":"Overview","text":"<p>This document provides an overview of the Unix file permission model, which governs access to files and directories on REPACSS systems. Each object in the file system is associated with an owner and permission flags that define read, write, and execute privileges for different user classes.</p> <p>Permissions are typically reviewed using the <code>ls -l</code> command:</p> <pre><code>ls -l\n</code></pre> <p>Example output:</p> <pre><code>drwx------ 2 bsencer bsencer  2048 Jun 12 2012 private\n-rw------- 1 bsencer bsencer  1327 Apr  9 2012 try.f90\n-rwx------ 1 bsencer bsencer 12040 Apr  9 2012 a.out\ndrwxr-xr-x 3 bsencer bsencer  2048 Nov 13 2011 public\n</code></pre>"},{"location":"understanding/repacss-system/file-system/unix-permissions.html#file-permission-structure","title":"File Permission Structure","text":"<p>Each line of output in <code>ls -l</code> contains a 10-character string representing the file type and permission bits:</p> Position Description 1 File type: <code>d</code> (directory) or <code>-</code> (regular file) 2\u20134 Permissions for the user (owner) 5\u20137 Permissions for the group (not evaluated in current REPACSS scope) 8\u201310 Permissions for others (i.e., all users) <p>Permission flags are defined as follows:</p> <ul> <li><code>r</code>: Read permission</li> <li><code>w</code>: Write permission</li> <li><code>x</code>: Execute permission</li> <li><code>-</code>: Permission not granted</li> </ul> <p>Special flags such as <code>s</code> (setgid) are not used in standard user scenarios on REPACSS.</p>"},{"location":"understanding/repacss-system/file-system/unix-permissions.html#interpreting-permission-strings","title":"Interpreting Permission Strings","text":"<ul> <li><code>drwx------</code>: A directory accessible only by the owner</li> <li><code>-rw-------</code>: A file readable and writable only by the owner</li> <li><code>-rwx------</code>: An executable file restricted to the owner</li> <li><code>drwxr-xr-x</code>: A directory accessible to all users for reading and execution, but writable only by the owner</li> </ul>"},{"location":"understanding/repacss-system/file-system/unix-permissions.html#managing-default-permissions-umask","title":"Managing Default Permissions: <code>umask</code>","text":"<p>The <code>umask</code> command controls the default permission settings for newly created files and directories. The following table outlines common <code>umask</code> values and their corresponding default permissions:</p> <code>umask</code> File Permissions Directory Permissions 002 <code>rw-rw-r--</code> <code>rwxrwxr-x</code> 007 <code>rw-rw----</code> <code>rwxrwx---</code> 022 <code>rw-r--r--</code> <code>rwxr-xr-x</code> 027 <code>rw-r-----</code> <code>rwxr-x---</code> 077 <code>rw-------</code> <code>rwx------</code> <p>Users may configure their preferred <code>umask</code> value by adding the command to their <code>.bash_profile</code>.</p>"},{"location":"understanding/repacss-system/file-system/unix-permissions.html#modifying-permissions-chmod","title":"Modifying Permissions: <code>chmod</code>","text":"<p>The <code>chmod</code> utility is used to alter file and directory permissions. This can be done using either octal or symbolic notation.</p>"},{"location":"understanding/repacss-system/file-system/unix-permissions.html#octal-notation","title":"Octal Notation","text":"Octal Value Permission Bits Description 0 <code>---</code> No permissions 1 <code>--x</code> Execute only 2 <code>-w-</code> Write only 3 <code>-wx</code> Write and execute 4 <code>r--</code> Read only 5 <code>r-x</code> Read and execute 6 <code>rw-</code> Read and write 7 <code>rwx</code> All permissions <p>Example:</p> <pre><code>chmod 755 file\n</code></pre> <p>This sets the file to be fully accessible by the owner, and readable and executable by others (<code>rwxr-xr-x</code>).</p>"},{"location":"understanding/repacss-system/file-system/unix-permissions.html#symbolic-notation","title":"Symbolic Notation","text":"<p>Users may also modify permissions with symbolic notation:</p> <pre><code>chmod u+x,go+rx file\n</code></pre> <p>This command grants execute permission to the user, and read/execute permissions to group and others.</p> Class Definition <code>u</code> User (owner) <code>o</code> Other (non-owners) <code>a</code> All users Operator Meaning <code>+</code> Add permission <code>-</code> Remove permission <code>=</code> Set exact permission Mode Action <code>r</code> Read <code>w</code> Write <code>x</code> Execute <code>X</code> Conditional execute (directories or if already executable) <p>Recursive operations may be executed as follows:</p> <pre><code>chmod -R o+rX directory/\n</code></pre> <p>This command grants read and execute access to others for all applicable files and directories within the specified directory.</p>"},{"location":"understanding/repacss-system/file-system/unix-permissions.html#summary","title":"Summary","text":"<ul> <li>Use <code>ls -l</code> to audit file and directory permissions.</li> <li>Apply <code>chmod</code> to update access control.</li> <li>Configure <code>umask</code> to enforce default file security.</li> <li>Sensitive files should be explicitly protected using:</li> </ul> <pre><code>chmod 600 sensitive_file\n</code></pre> <p>This ensures exclusive read/write access to the owner and denies access to all other users.</p>"}]}